{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as op\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import mne_nirs\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import sem\n",
    "from scipy import stats\n",
    "from itertools import compress\n",
    "from pprint import pprint\n",
    "from nilearn.plotting import plot_design_matrix\n",
    "\n",
    "from mne.viz import plot_compare_evokeds\n",
    "from mne.preprocessing.nirs import (optical_density,\n",
    "                                    temporal_derivative_distribution_repair)\n",
    "\n",
    "from mne_nirs.visualisation import plot_glm_surface_projection\n",
    "from mne_nirs.experimental_design import make_first_level_design_matrix\n",
    "from mne_nirs.statistics import run_glm\n",
    "from mne_nirs.channels import (get_long_channels,\n",
    "                               get_short_channels,\n",
    "                               picks_pair_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WAA analysis\n",
    "def weighted_averaging_analysis(raw_haemo):\n",
    "\n",
    "    # remove short channels\n",
    "    raw_haemo = mne_nirs.channels.get_long_channels(raw_haemo)\n",
    "    \n",
    "    # divide events \n",
    "    events, event_dict = mne.events_from_annotations(raw_haemo,{'Control': 1, 'Noise': 2, 'Speech': 3})\n",
    "\n",
    "    # time frame for epochs\n",
    "    tmin, tmax = -5, 15\n",
    "\n",
    "    # create epochs\n",
    "    epochs = mne.Epochs(raw_haemo, events, event_id=event_dict,\n",
    "                        tmin=tmin, tmax=tmax,\n",
    "                        reject_by_annotation=True,\n",
    "                        proj=True, baseline=(None, 0), preload=True,\n",
    "                        detrend=1, verbose=True)\n",
    "    \n",
    "    \n",
    "\n",
    "    # PAIRED T TEST\n",
    "    # create arrays for each condition + channelname. roll 2 to get source 12 to its ROI\n",
    "    chnames = np.roll(epochs.copy().ch_names,2)\n",
    "    carray = np.roll(np.mean(epochs['Control'].copy().crop(4,6)._data,axis=2),2,axis=1)\n",
    "    narray = np.roll(np.mean(epochs['Noise'].copy().crop(4,6)._data,axis=2),2,axis=1)\n",
    "    sarray = np.roll(np.mean(epochs['Speech'].copy().crop(4,6)._data,axis=2),2,axis=1)\n",
    "\n",
    "    maxlen = max([len(carray),len(narray),len(sarray)])\n",
    "\n",
    "    # replace missing values with mean of array\n",
    "    while len(carray) < maxlen:\n",
    "        carray = np.vstack([carray, np.mean(carray,axis=0)])\n",
    "\n",
    "    while len(narray) < maxlen:\n",
    "        narray = np.vstack([narray, np.mean(narray,axis=0)])\n",
    "\n",
    "    while len(sarray) < maxlen:\n",
    "        sarray = np.vstack([sarray, np.mean(sarray,axis=0)])\n",
    "\n",
    "    # difference in control : active stimuli\n",
    "    ncont = narray - carray\n",
    "    scont = sarray - carray\n",
    "\n",
    "    # calculate t-values\n",
    "    m = len(chnames)\n",
    "    speechp = np.empty(m)\n",
    "    noisep = np.empty(m)\n",
    "\n",
    "    for ii in range(0,m):\n",
    "        speechp[ii] = stats.t.sf(abs(np.mean(scont[:,ii])/sem(scont[:,ii])), maxlen-1)\n",
    "        noisep[ii] = stats.t.sf(abs(np.mean(ncont[:,ii])/sem(ncont[:,ii])), maxlen-1)\n",
    "\n",
    "\n",
    "    # FDR CORRECTION\n",
    "    WAAres = pd.DataFrame()\n",
    "    WAAres['Channels'] = chnames\n",
    "    WAAres['Noise'] = noisep\n",
    "    WAAres['Speech'] = speechp\n",
    "    \n",
    "    # rank array and alpha value\n",
    "    FDRarray = np.array(range(1,m+1))\n",
    "    FDRalpha = 0.20\n",
    "\n",
    "    # array containing the comparison value\n",
    "    FDRarray = FDRalpha * FDRarray / m\n",
    "\n",
    "    for jj,cond in enumerate(['Noise','Speech']):\n",
    "        # ranking by smallest p-value\n",
    "        WAAres = WAAres.sort_values(by=cond, ascending=True)\n",
    "        # comparing value to array\n",
    "        WAAres[cond] = WAAres[cond] < FDRarray\n",
    "        maxpos = WAAres[cond][::-1].idxmax()\n",
    "        minpos = WAAres[cond][::-1].idxmin()\n",
    "        if maxpos == minpos:\n",
    "            continue\n",
    "        maxloc = WAAres.index.get_loc(maxpos)\n",
    "        # setting all lower ranks to true\n",
    "        WAAres[cond].iloc[0:maxloc + 1] = True\n",
    "\n",
    "    # sorting by channels again\n",
    "    WAAres = WAAres.sort_index()\n",
    "    \n",
    "\n",
    "    left = [[4, 2], [4, 3], [5, 2], [5, 3], [5, 4], [5, 5]] #, [4, 14], [5, 15]\n",
    "    right = [[10, 9], [10, 10], [10, 11], [10, 12], [11, 11], [11, 12]] #, [10, 18], [11, 19]\n",
    "    back = [[6, 6], [6, 8], [7, 6], [7, 7], [7, 8], [8, 7], [8, 8], [9, 8]] #, [6, 16], [8, 17]\n",
    "    front = [[1, 1], [2, 1], [3, 1], [3, 2], [12, 1]] #, [2, 13], [12, 20]\n",
    "\n",
    "    ROIs = dict(STG=np.concatenate((np.array(picks_pair_to_idx(raw_haemo, left))/2//1,\n",
    "                np.array(picks_pair_to_idx(raw_haemo, right))/2//1)),\n",
    "                Occipital=np.array(picks_pair_to_idx(raw_haemo, back))/2//1,\n",
    "                Frontal=np.array(picks_pair_to_idx(raw_haemo, front))/2//1)\n",
    "\n",
    "    ROIs['STG'] = ROIs['STG'].astype(int)\n",
    "    ROIs['Occipital'] = ROIs['Occipital'].astype(int)\n",
    "    ROIs['Frontal'] = ROIs['Frontal'].astype(int)\n",
    "\n",
    "    evoked_dict = {'Noise/HbO': epochs['Noise'].average(picks='hbo'),\n",
    "                'Noise/HbR': epochs['Noise'].average(picks='hbr'),\n",
    "                'Speech/HbO': epochs['Speech'].average(picks='hbo'),\n",
    "                'Speech/HbR': epochs['Speech'].average(picks='hbr'),\n",
    "                'Control/HbO': epochs['Control'].average(picks='hbo'),\n",
    "                'Control/HbR': epochs['Control'].average(picks='hbr')}  \n",
    "\n",
    "\n",
    "    for condition in evoked_dict:\n",
    "        evoked_dict[condition].rename_channels(lambda x: x[:-4])\n",
    "\n",
    "    condlist=['Control','Noise','Speech']\n",
    "\n",
    "    #fig, axes = plt.subplots(nrows=len(ROIs), ncols=len(event_dict),figsize=(14, 10))\n",
    "\n",
    "    WaaRoiGrp = pd.DataFrame()\n",
    "\n",
    "\n",
    "    for ridx, roi in enumerate(ROIs):\n",
    "        for cidx, event in enumerate(event_dict): \n",
    "            plot_compare_evokeds([evoked_dict[condlist[cidx]+'/HbO'],evoked_dict[condlist[cidx]+'/HbR']], picks=ROIs[roi], combine=\"mean\", title='', \n",
    "                axes=axes[ridx,cidx], show=False,colors=['r','b'], ylim=dict(hbo=[-0.1, 0.2],hbr=[-0.1, 0.2]), legend=False, show_sensors=True, ci=0.95)\n",
    "            \n",
    "            axes[0,cidx].set_title(f'{event}')\n",
    "            axes[ridx, 0].set_ylabel(f\"{roi}\\n Î¼M\")\n",
    "            \n",
    "            if cidx == 0:\n",
    "                if ridx ==0: \n",
    "                    contvalhbo = evoked_dict[condlist[cidx]+'/HbO'].copy()\n",
    "                    contvalhbr = evoked_dict[condlist[cidx]+'/HbR'].copy()\n",
    "                contvalhbo._data = contvalhbo._data + evoked_dict[condlist[cidx]+'/HbO'].copy()._data\n",
    "                contvalhbr._data = contvalhbr._data + evoked_dict[condlist[cidx]+'/HbR'].copy()._data\n",
    "\n",
    "\n",
    "            if cidx == 1:\n",
    "                if ridx ==0: \n",
    "                    noisevalhbo = evoked_dict[condlist[cidx]+'/HbO'].copy()\n",
    "                    noisevalhbr = evoked_dict[condlist[cidx]+'/HbR'].copy()\n",
    "                noisevalhbo._data = noisevalhbo._data + evoked_dict[condlist[cidx]+'/HbO'].copy()._data\n",
    "                noisevalhbr._data = noisevalhbr._data + evoked_dict[condlist[cidx]+'/HbR'].copy()._data\n",
    "\n",
    "            if cidx == 2:\n",
    "                if ridx ==0: \n",
    "                    speechvalhbo = evoked_dict[condlist[cidx]+'/HbO'].copy()\n",
    "                    speechvalhbr = evoked_dict[condlist[cidx]+'/HbR'].copy()\n",
    "                speechvalhbo._data = speechvalhbo._data + evoked_dict[condlist[cidx]+'/HbO'].copy()._data\n",
    "                speechvalhbr._data = speechvalhbr._data + evoked_dict[condlist[cidx]+'/HbR'].copy()._data\n",
    "\n",
    "    WaaRoiGrp={}\n",
    "    WaaRoiGrp['ControlHbO'] = contvalhbo\n",
    "    WaaRoiGrp['ControlHbR'] = contvalhbr\n",
    "    WaaRoiGrp['NoiseHbO'] = noisevalhbo\n",
    "    WaaRoiGrp['NoiseHbR'] = noisevalhbr\n",
    "    WaaRoiGrp['SpeechHbO'] = speechvalhbo\n",
    "    WaaRoiGrp['SpeechHbR'] = speechvalhbr\n",
    "\n",
    "\n",
    "    axes[0, 0].legend([\"Oxyhaemoglobin\", \"Deoxyhaemoglobin\"],loc=2)\n",
    "\n",
    "\n",
    "\n",
    "    color_dict = dict(HbO='#AA3377', HbR='b')\n",
    "    styles_dict = dict(Control=dict(linestyle='dotted'), Noise=dict(linestyle='dashed'))\n",
    "\n",
    "    mne.viz.plot_compare_evokeds(evoked_dict, combine=\"mean\", ci=0.95,\n",
    "                                colors=color_dict, styles=styles_dict,show=True)\n",
    "    \n",
    "    return WAAres, WaaRoiGrp, ROIs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GLM_analysis(raw_haemo):\n",
    "\n",
    "    # removing break annotations\n",
    "    unwanted = np.nonzero(raw_haemo.annotations.description == 'Xstart')\n",
    "    raw_haemo.annotations.delete(unwanted)\n",
    "    unwanted = np.nonzero(raw_haemo.annotations.description == 'Xend')\n",
    "    raw_haemo.annotations.delete(unwanted)\n",
    "\n",
    "    short_chs = get_short_channels(raw_haemo)\n",
    "    raw_haemo = get_long_channels(raw_haemo)\n",
    "\n",
    "\n",
    "    events, event_dict = mne.events_from_annotations(raw_haemo, verbose=False)\n",
    "    mne.viz.plot_events(events, event_id=event_dict, sfreq=raw_haemo.info['sfreq'],show=False)\n",
    "\n",
    "\n",
    "    s = mne_nirs.experimental_design.create_boxcar(raw_haemo)\n",
    "    \"\"\" fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15, 6))\n",
    "    plt.plot(raw_haemo.times, s, axes=axes)\n",
    "    plt.legend([\"Control\", \"Noise\", \"Speech\"], loc=\"upper right\")\n",
    "    plt.xlabel(\"Time (s)\") \"\"\"\n",
    "\n",
    "\n",
    "    raw_stim = raw_haemo.copy()\n",
    "    raw_stim.annotations.delete(raw_stim.annotations.description == 'Control')\n",
    "\n",
    "\n",
    "    isis, names = mne_nirs.experimental_design.longest_inter_annotation_interval(raw_stim)\n",
    "\n",
    "\n",
    "\n",
    "    design_matrix = make_first_level_design_matrix(raw_stim,\n",
    "                                                drift_model='cosine',\n",
    "                                                high_pass=1/(2*max(isis)),  # Must be specified per experiment\n",
    "                                                hrf_model='spm',\n",
    "                                                stim_dur=5.125)\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "    #design_matrix[\"ShortHbO\"] = np.mean(short_chs.copy().pick(\n",
    "    #                                    picks=\"hbo\").get_data(), axis=0)\n",
    "\n",
    "    #design_matrix[\"ShortHbR\"] = np.mean(short_chs.copy().pick(\n",
    "    #                                    picks=\"hbr\").get_data(), axis=0)\n",
    "\n",
    "\n",
    "    \"\"\" fig, ax1 = plt.subplots(figsize=(10, 6), nrows=1, ncols=1)\n",
    "    fig = plot_design_matrix(design_matrix, ax=ax1) \"\"\"\n",
    "\n",
    "\n",
    "    glm_est = run_glm(raw_haemo, design_matrix, noise_model='auto')\n",
    "\n",
    "\n",
    "    subjects_dir = op.join(mne.datasets.sample.data_path(), 'subjects')\n",
    "    mne.utils.set_config(\"SUBJECTS_DIR\", subjects_dir, set_env=True)\n",
    "    \n",
    "\n",
    "    GLMresN = pd.DataFrame()\n",
    "    GLMresS = pd.DataFrame()\n",
    "    df2 = glm_est.to_dataframe()\n",
    "\n",
    "\n",
    "    GLMresN['ch_name'] = df2['ch_name'][df2.Condition == 'Noise']\n",
    "    GLMresN['Noise'] = df2['p_value'][df2.Condition == 'Noise']\n",
    "    GLMresN = GLMresN.reset_index(drop=True)\n",
    "    GLMresS['ch_name'] = df2['ch_name'][df2.Condition == 'Speech']\n",
    "    GLMresS['Speech'] = df2['p_value'][df2.Condition == 'Speech']\n",
    "    GLMresS = GLMresS.reset_index(drop=True)\n",
    "\n",
    "    GLMres = pd.merge(GLMresN,GLMresS, on = 'ch_name')\n",
    "\n",
    "    m = len(GLMres)\n",
    "    FDRarray = np.array(range(1,m+1))\n",
    "    FDRalpha = 0.2\n",
    "    \n",
    "\n",
    "    FDRarray = FDRalpha * FDRarray / m\n",
    "\n",
    "    for jj,cond in enumerate(['Noise','Speech']):\n",
    "        GLMres = GLMres.sort_values(by=cond, ascending=True)\n",
    "        GLMres[cond] = GLMres[cond] < FDRarray\n",
    "        maxpos = GLMres[cond][::-1].idxmax()\n",
    "        minpos = GLMres[cond][::-1].idxmin()\n",
    "        if maxpos == minpos:\n",
    "            continue\n",
    "        maxloc = GLMres.index.get_loc(maxpos)\n",
    "        GLMres[cond].iloc[0:maxloc + 1] = True\n",
    "\n",
    "\n",
    "\n",
    "    GLMres = GLMres.sort_index()\n",
    "\n",
    "    #mne.viz.plot_events(events, event_id=event_dict, sfreq=raw_haemo.info['sfreq'],show=True)\n",
    "\n",
    "\n",
    "    return GLMres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fullanalysis(SubjectID,VisitID):\n",
    "\n",
    "    idxstring = np.array([\n",
    "        [4, 7],   # For Subject 1\n",
    "        [9, 7],   # For Subject 2\n",
    "        [13, 11], # For Subject 3\n",
    "        [5, 5],   # For Subject 4\n",
    "        [1, 1],   # For Subject 5\n",
    "        [1, 1],   # For Subject 6\n",
    "        [1, 1],   # For Subject 7\n",
    "        [1, 1],   # For Subject 8\n",
    "        [1, 1],   # For Subject 9\n",
    "        [1, 1],   # For Subject 10\n",
    "        [1, 1],   # For Subject 11\n",
    "        [1, 1],   # For Subject 12\n",
    "        [1, 1],   # For Subject 13\n",
    "        [1, 1],   # For Subject 14\n",
    "        [1, 1],   # For Subject 15\n",
    "        [1, 1],   # For Subject 16\n",
    "        [1, 1],   # For Subject 17\n",
    "        [1, 1],   # For Subject 18\n",
    "        [1, 1],   # For Subject 19\n",
    "        [1, 1],   # For Subject 20\n",
    "        [1, 1],   # For Subject 21\n",
    "        [1, 1],   # For Subject 22\n",
    "        [1, 1],   # For Subject 23\n",
    "        [1, 1]    # For Subject 24\n",
    "    ])\n",
    "\n",
    "\n",
    "    BadEpochList = np.empty((24,2,13),dtype=int)\n",
    "    BadEpochList[1-1][1-1][0:4]=[2-1, 18-1, 37-1, 54-1]\n",
    "    BadEpochList[1-1][2-1][0:7]=[8-1, 15-1, 16-1, 17-1, 28-1, 41-1, 60-1]\n",
    "    BadEpochList[2-1][1-1][0:9]=[3-1, 5-1, 9-1, 16-1, 30-1, 37-1, 38-1, 51-1, 53-1]\n",
    "    BadEpochList[2-1][2-1][0:7]=[7-1, 8-1, 10-1, 12-1, 51-1, 56-1, 57-1]\n",
    "    BadEpochList[3-1][1-1][0:13]=[5-1, 8-1, 9-1, 16-1, 18-1, 19-1, 26-1, 33-1, 46-1, 47-1, 49-1, 57-1, 58-1]\n",
    "    BadEpochList[3-1][2-1][0:11]=[1-1, 3-1, 7-1, 13-1, 16-1, 21-1, 35-1, 46-1, 54-1, 57-1, 60-1]\n",
    "    BadEpochList[4-1][1-1][0:5]=[7-1, 8-1, 20-1, 41-1, 44-1]\n",
    "    BadEpochList[4-1][2-1][0:5]=[33-1, 36-1, 38-1, 40-1, 56-1]\n",
    "    BadEpochList[5-1][1-1][0:1]=[60-1]\n",
    "    BadEpochList[5-1][2-1][0:1]=[60-1]\n",
    "    BadEpochList[6-1][1-1][0:1]=[60-1]\n",
    "    BadEpochList[6-1][2-1][0:1]=[60-1]\n",
    "    BadEpochList[7-1][1-1][0:1]=[60-1]\n",
    "    BadEpochList[7-1][2-1][0:1]=[60-1]\n",
    "    BadEpochList[8-1][1-1][0:1]=[60-1]\n",
    "    BadEpochList[8-1][2-1][0:1]=[60-1]\n",
    "    BadEpochList[9-1][1-1][0:1]=[60-1]\n",
    "    BadEpochList[9-1][2-1][0:1]=[60-1]\n",
    "    BadEpochList[10-1][1-1][0:1]=[60-1]\n",
    "    BadEpochList[10-1][2-1][0:1]=[60-1]\n",
    "    BadEpochList[11-1][1-1][0:1]=[60-1]\n",
    "    BadEpochList[11-1][2-1][0:1]=[60-1]\n",
    "    BadEpochList[12-1][1-1][0:1]=[60-1]\n",
    "    BadEpochList[12-1][2-1][0:1]=[60-1]\n",
    "    BadEpochList[13-1][1-1][0:1]=[60-1]\n",
    "    BadEpochList[13-1][2-1][0:1]=[60-1]\n",
    "    BadEpochList[14-1][1-1][0:1]=[60-1]\n",
    "    BadEpochList[14-1][2-1][0:1]=[60-1]\n",
    "    BadEpochList[15-1][1-1][0:1]=[60-1]\n",
    "    BadEpochList[15-1][2-1][0:1]=[60-1]\n",
    "    BadEpochList[16-1][1-1][0:1]=[60-1]\n",
    "    BadEpochList[16-1][2-1][0:1]=[60-1]\n",
    "    BadEpochList[17-1][1-1][0:1]=[60-1]\n",
    "    BadEpochList[17-1][2-1][0:1]=[60-1]\n",
    "    BadEpochList[18-1][1-1][0:1]=[60-1]\n",
    "    BadEpochList[18-1][2-1][0:1]=[60-1]\n",
    "    BadEpochList[19-1][1-1][0:1]=[60-1]\n",
    "    BadEpochList[19-1][2-1][0:1]=[60-1]\n",
    "    BadEpochList[20-1][1-1][0:1]=[60-1]\n",
    "    BadEpochList[20-1][2-1][0:1]=[60-1]\n",
    "    BadEpochList[21-1][1-1][0:1]=[60-1]\n",
    "    BadEpochList[21-1][2-1][0:1]=[60-1]\n",
    "    BadEpochList[22-1][1-1][0:1]=[60-1]\n",
    "    BadEpochList[22-1][2-1][0:1]=[60-1]\n",
    "    BadEpochList[23-1][1-1][0:1]=[60-1]\n",
    "    BadEpochList[23-1][2-1][0:1]=[60-1]\n",
    "    BadEpochList[24-1][1-1][0:1]=[60-1]\n",
    "    BadEpochList[24-1][2-1][0:1]=[60-1]\n",
    "\n",
    "\n",
    "    #fnirs_data_folder =(r'C:\\Users\\Andy\\OneDrive - Danmarks Tekniske Universitet\\Data\\Recordings')\n",
    "    # New base path\n",
    "    fnirs_data_folder = r'C:\\Users\\sarab\\Desktop\\THESIS\\Tutorial MNE\\subjects'\n",
    "\n",
    "    # Build subject and session names based on ID\n",
    "    subject = f\"sub-{SubjectID:02d}\"\n",
    "    session = f\"ses-{VisitID:02d}\"\n",
    "    nirs_folder = 'nirs'\n",
    "\n",
    "    fnirs_cw_amplitude_dir = op.join(fnirs_data_folder, subject, session, nirs_folder)\n",
    "\n",
    "    raw_intensity = mne.io.read_raw_nirx(fnirs_cw_amplitude_dir)\n",
    "    raw_intensity.load_data()\n",
    "    #raw_intensity.info['bads'].extend(['S3_D2 785', 'S3_D2 830'])\n",
    "\n",
    "    # annotation names\n",
    "    raw_intensity.annotations.rename({'1.0': 'Control',\n",
    "                                    '2.0': 'Noise',\n",
    "                                    '3.0': 'Speech',\n",
    "                                    '4.0': 'Xstart',\n",
    "                                    '5.0': 'Xend'})\n",
    "\n",
    "    # annotation durations\n",
    "    raw_intensity.annotations.set_durations({'Control' : 5, 'Noise' : 5, 'Speech' : 5.25})\n",
    "\n",
    "    # Break events\n",
    "    Breaks, event_dict = mne.events_from_annotations(raw_intensity,{'Xend': 4, 'Xstart': 5})\n",
    "    # All events\n",
    "    AllEvents, event_dict = mne.events_from_annotations(raw_intensity)\n",
    "\n",
    "    # Converting from index to time\n",
    "    Breaks = Breaks[:,0]/raw_intensity.info['sfreq']\n",
    "    LastEvent = AllEvents[-1,0]/raw_intensity.info['sfreq']\n",
    "\n",
    "\n",
    "    # constructing block for each block in experiment\n",
    "    cropped_intensity = raw_intensity.copy().crop(Breaks[0],Breaks[1])\n",
    "    block2 = raw_intensity.copy().crop(Breaks[2],Breaks[3])\n",
    "    block3 = raw_intensity.copy().crop(Breaks[4],Breaks[5])\n",
    "    block4 = raw_intensity.copy().crop(Breaks[6], LastEvent+15.25)\n",
    "\n",
    "    # Combining all blocks\n",
    "    cropped_intensity.append([block2,block3,block4])\n",
    "    cropped_od = mne.preprocessing.nirs.optical_density(cropped_intensity)\n",
    "    cropped_corrected_od = temporal_derivative_distribution_repair(cropped_od)\n",
    "\n",
    "\n",
    "    sci = mne.preprocessing.nirs.scalp_coupling_index(cropped_corrected_od)\n",
    "    \"\"\" fig, ax = plt.subplots()\n",
    "    ax.hist(sci)\n",
    "    ax.set(xlabel='Scalp Coupling Index', ylabel='Count', xlim=[0, 1])\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # removing break annotations\n",
    "    unwanted = np.nonzero(raw_intensity.annotations.description == 'Xstart')\n",
    "    raw_intensity.annotations.delete(unwanted)\n",
    "    unwanted = np.nonzero(raw_intensity.annotations.description == 'Xend')\n",
    "    raw_intensity.annotations.delete(unwanted)\n",
    "\n",
    "    #raw_intensity.annotations.delete(BadEpochList[SubjectID-1][VisitID-1][0:idxstring[SubjectID-1,VisitID-1]])\n",
    "\n",
    "\n",
    "    raw_od = mne.preprocessing.nirs.optical_density(raw_intensity)\n",
    "\n",
    "\n",
    "    corrected_od = temporal_derivative_distribution_repair(raw_od)\n",
    "\n",
    "\n",
    "    corrected_od.info['bads'] = list(compress(cropped_corrected_od.ch_names, sci < 0.7))\n",
    "    #corrected_od.info['bads'].extend(['S3_D2 785', 'S3_D2 830'])\n",
    "    \n",
    "    raw_haemo = mne.preprocessing.nirs.beer_lambert_law(corrected_od, ppf=0.1)\n",
    "    raw_haemo.plot(n_channels=len(raw_haemo.ch_names),\n",
    "                duration=500, show_scrollbars=False,show=False)\n",
    "\n",
    "\n",
    "    \"\"\" fig = raw_haemo.plot_psd(average=True,show=False)\n",
    "    fig.suptitle('Before filtering', weight='bold', size='x-large')\n",
    "    fig.subplots_adjust(top=0.88) \"\"\"\n",
    "\n",
    "\n",
    "    raw_haemo = raw_haemo.filter(l_freq = None, h_freq = 0.2,  \n",
    "                                 method=\"iir\", iir_params =dict(order=5, ftype='butter'))\n",
    "     #high-pass\n",
    "    raw_haemo= raw_haemo.filter(l_freq =  0.005, h_freq = None, method=\"iir\", iir_params =dict(order=5, ftype='butter'))\n",
    "    \n",
    "    \"\"\" fig = raw_haemo.plot_psd(average=True,show=False)\n",
    "    fig.suptitle('After filtering', weight='bold', size='x-large')\n",
    "    fig.subplots_adjust(top=0.88) \"\"\"\n",
    "\n",
    "\n",
    "    raw_haemo.plot(n_channels=len(raw_haemo.ch_names),\n",
    "                duration=500, show_scrollbars=False,show=False)\n",
    "\n",
    "\n",
    "    WAAres,WaaGrpDF, ROIs = weighted_averaging_analysis(raw_haemo)\n",
    "\n",
    "    GLMres = GLM_analysis(raw_haemo)\n",
    "    return WAAres, WaaGrpDF, ROIs, GLMres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading C:\\Users\\sarab\\Desktop\\THESIS\\Tutorial MNE\\subjects\\sub-05\\ses-01\\nirs\n",
      "Reading 0 ... 8666  =      0.000 ...  1663.872 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sarab\\AppData\\Local\\Temp\\ipykernel_38120\\1544337469.py:93: RuntimeWarning: Extraction of measurement date from NIRX file failed. This can be caused by files saved in certain locales (currently only ['en_US.utf8', 'de_DE', 'fr_FR', 'it_IT'] supported). Please report this as a github issue. The date is being set to January 1st, 2000, instead of '\"27. jun 2023\"\"16:28:00.309\"'.\n",
      "  raw_intensity = mne.io.read_raw_nirx(fnirs_cw_amplitude_dir)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Annotations descriptions: [np.str_('Xend'), np.str_('Xstart')]\n",
      "Used Annotations descriptions: [np.str_('Control'), np.str_('Noise'), np.str_('Speech'), np.str_('Xend'), np.str_('Xstart')]\n",
      "Using qt as 2D backend.\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up low-pass filter at 0.2 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth lowpass zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 10 (effective, after forward-backward)\n",
      "- Cutoff at 0.20 Hz: -6.02 dB\n",
      "\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up high-pass filter at 0.005 Hz\n",
      "\n",
      "IIR filter parameters\n",
      "---------------------\n",
      "Butterworth highpass zero-phase (two-pass forward and reverse) non-causal filter:\n",
      "- Filter order 10 (effective, after forward-backward)\n",
      "- Cutoff at 0.01 Hz: -6.02 dB\n",
      "\n",
      "Used Annotations descriptions: [np.str_('Control'), np.str_('Noise'), np.str_('Speech')]\n",
      "Not setting metadata\n",
      "60 matching events found\n",
      "Setting baseline interval to [-4.992000319488021, 0.0] s\n",
      "Applying baseline correction (mode: mean)\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 60 events and 105 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "WAAres = {}    \n",
    "WaaRoiGrp = {}\n",
    "GLMres = {}\n",
    "\n",
    "roilist = ['front', 'left', 'back', 'right']\n",
    "\n",
    "\n",
    "# run analysis and save results in csv file\n",
    "# Range (1,5) is actually = 1,2,3,4\n",
    "\n",
    " # whatever subjects you want\n",
    "\n",
    "\n",
    "for SubjectID in range(5,6):\n",
    "    if SubjectID == 9:\n",
    "        continue\n",
    "    for VisitID in range (1,2):\n",
    "        dfwaa, WaaGrpDF, ROIs, dfglm = fullanalysis(SubjectID,VisitID)\n",
    "        dfwaa.to_csv('WAA' + str(SubjectID) + str(VisitID) + '.csv', index=False)\n",
    "        dfglm.to_csv('GLM' + str(SubjectID) + str(VisitID) + '.csv', index=False)\n",
    "        WaaRoiGrp[(SubjectID-1)*2+VisitID] = WaaGrpDF\n",
    "\n",
    "# For group level waa plot,\n",
    "\n",
    "# Epochs are probably not saved correctly from the analysis, the 95% confidence interval does not work here.\n",
    "# Find a better way to store the epochs.\n",
    "condlist =  [['ControlHbO','ControlHbR'],['NoiseHbO','NoiseHbR'],['SpeechHbO','SpeechHbR']]\n",
    "\n",
    "for test in range (2,9):\n",
    "    for condidx, cond in enumerate(condlist):\n",
    "        WaaRoiGrp[1][cond[0]]._data = WaaRoiGrp[1][cond[0]]._data + WaaRoiGrp[test][cond[0]]._data\n",
    "        WaaRoiGrp[1][cond[1]]._data = WaaRoiGrp[1][cond[1]]._data + WaaRoiGrp[test][cond[1]]._data\n",
    "\n",
    "\n",
    "for ii in range(len(condlist)):\n",
    "    WaaRoiGrp[1][condlist[ii][0]]._data = WaaRoiGrp[1][condlist[ii][0]]._data /8\n",
    "    WaaRoiGrp[1][condlist[ii][1]]._data = WaaRoiGrp[1][condlist[ii][1]]._data /8\n",
    "\n",
    "fig, axes = plt.subplots(nrows=len(ROIs), ncols=len(condlist),figsize=(14, 10))\n",
    "titlestr = ['Control','Noise','Speech']\n",
    "for ridx, roi in enumerate(ROIs):\n",
    "    for cidx, event in enumerate(condlist): \n",
    "        plot_compare_evokeds([WaaRoiGrp[1][event[0]],WaaRoiGrp[1][event[1]]], picks=ROIs[roi], combine=\"mean\", title='', \n",
    "            axes=axes[ridx,cidx], show=False,colors=['r','b'], ylim=dict(hbo=[-0.2, 0.4],hbr=[-0.2, 0.4]), legend=False, show_sensors=True, ci=0.95)\n",
    "        \n",
    "        axes[0,cidx].set_title(f'{titlestr[cidx]}')\n",
    "        axes[ridx, 0].set_ylabel(f\"{roi}\\n Î¼M\")\n",
    "\n",
    "axes[0, 0].legend([\"Oxyhaemoglobin\", \"Deoxyhaemoglobin\"],loc=2)\n",
    "\n",
    "\n",
    "\n",
    "print('dont stop debugging')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
