{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sphinx_gallery_thumbnail_number = 2\n",
    "\n",
    "# Authors: Robert Luke <mail@robertluke.net>\n",
    "#\n",
    "# License: BSD (3-clause)\n",
    "\n",
    "# Import common libraries\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from itertools import compress\n",
    "from pprint import pprint\n",
    "\n",
    "# Import Plotting Library\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import pywt\n",
    "import mne_nirs\n",
    "import mne\n",
    "\n",
    "\n",
    "\n",
    "# Import StatsModels\n",
    "import statsmodels.formula.api as smf\n",
    "from mne import Epochs, events_from_annotations, set_log_level\n",
    "from mne.preprocessing.nirs import (\n",
    "    beer_lambert_law,\n",
    "    optical_density,\n",
    "    scalp_coupling_index,\n",
    "    temporal_derivative_distribution_repair,\n",
    "    \n",
    ")\n",
    "\n",
    "# Import MNE processing\n",
    "from mne.viz import plot_compare_evokeds\n",
    "\n",
    "# Import MNE-BIDS processing\n",
    "from mne_bids import BIDSPath, read_raw_bids\n",
    "\n",
    "\n",
    "# Import MNE-NIRS processing\n",
    "from mne_nirs.channels import get_long_channels, picks_pair_to_idx\n",
    "from mne_nirs.datasets import fnirs_motor_group\n",
    "from mne_nirs.signal_enhancement import (enhance_negative_correlation, short_channel_regression)\n",
    "from mne_nirs.channels import (get_long_channels,\n",
    "                               get_short_channels,\n",
    "                               picks_pair_to_idx)\n",
    "\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from itertools import compress\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from mne_nirs.experimental_design import make_first_level_design_matrix\n",
    "from mne_nirs.statistics import run_glm, statsmodels_to_results\n",
    "from nilearn.plotting import plot_design_matrix\n",
    "\n",
    "# Set general parameters\n",
    "set_log_level(\"WARNING\")  # Don't show info, as it is repetitive for many subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cropping the signal before sci calculation\n",
    "def preprocessing_glm_ROI(bids_path, subject_id, session_id, id):\n",
    "    # Read data with annotations in BIDS format\n",
    "    raw_intensity = read_raw_bids(bids_path=bids_path, verbose=False)\n",
    "    # check if coordinates of the channels\n",
    "    \n",
    "    \n",
    "    #print(raw_intensity.ch_names)\n",
    "\n",
    "    \n",
    "    # Get event timings\n",
    "    #print(\"Extracting event timings...\")\n",
    "    Breaks, _ = mne.events_from_annotations(raw_intensity, {'Xstart': 4, 'Xend': 5})\n",
    "    AllEvents, _ = mne.events_from_annotations(raw_intensity)\n",
    "    Breaks = Breaks[:, 0] / raw_intensity.info['sfreq']\n",
    "    LastEvent = AllEvents[-1, 0] / raw_intensity.info['sfreq']\n",
    "    \n",
    "    if len(Breaks) % 2 == 0:\n",
    "        raise ValueError(\"Breaks array should have an odd number of elements.\")\n",
    "    \n",
    "    original_duration = raw_intensity.times[-1] - raw_intensity.times[0]\n",
    "    #print(f\"Original duration: {original_duration:.2f} seconds\")\n",
    "    \n",
    "    # Cropping dataset\n",
    "    #print(\"Cropping the dataset...\")\n",
    "    cropped_intensity = raw_intensity.copy().crop(Breaks[0], Breaks[1])\n",
    "    for j in range(2, len(Breaks) - 1, 2):\n",
    "        block = raw_intensity.copy().crop(Breaks[j], Breaks[j + 1])\n",
    "        cropped_intensity.append(block)\n",
    "    cropped_intensity.append(raw_intensity.copy().crop(Breaks[-1], LastEvent + 15.25))\n",
    "    \n",
    "    cropped_duration = cropped_intensity.times[-1] - cropped_intensity.times[0]\n",
    "    #print(f\"Cropped duration: {cropped_duration:.2f} seconds\")\n",
    "    \n",
    "    if cropped_duration >= original_duration:\n",
    "        print(f\"WARNING: Cropping did not reduce duration!\")\n",
    "    \n",
    "    raw_intensity_cropped = cropped_intensity.copy()\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    # Remove break annotations\n",
    "    print(\"Removing break annotations for the orginal raw...\")\n",
    "    raw_intensity.annotations.delete(np.where(\n",
    "        (raw_intensity.annotations.description == 'Xstart') | \n",
    "        (raw_intensity.annotations.description == 'Xend') | \n",
    "        (raw_intensity.annotations.description == 'BAD boundary') | \n",
    "        (raw_intensity.annotations.description == 'EDGE boundary')\n",
    "    )[0])\n",
    "    \n",
    "    print(\"Removing break annotations for the cropped raw...\")\n",
    "    raw_intensity_cropped.annotations.delete(np.where(\n",
    "        (raw_intensity_cropped.annotations.description == 'Xstart') | \n",
    "        (raw_intensity_cropped.annotations.description == 'Xend') | \n",
    "        (raw_intensity_cropped.annotations.description == 'BAD boundary') | \n",
    "        (raw_intensity_cropped.annotations.description == 'EDGE boundary')\n",
    "    )[0]) \n",
    "    \n",
    "    \n",
    "    #downsampling the signal\n",
    "    #raw_intensity_cropped.resample(3)\n",
    "    \n",
    "    # Convert signal to optical density and determine bad channels\n",
    "    raw_od = optical_density(raw_intensity)\n",
    "    raw_od_cropped = optical_density(raw_intensity_cropped)\n",
    "    \n",
    "    # get the total number of short channels\n",
    "    short_chs = get_short_channels(raw_od)\n",
    "    tot_number_of_short_channels = len(short_chs.ch_names)\n",
    "    \n",
    "\n",
    "    # sci calculated\n",
    "    sci = scalp_coupling_index(raw_od_cropped, l_freq=0.7, h_freq=1.45)\n",
    "    #sci = scalp_coupling_index(raw_od_cropped, h_freq=1.35)\n",
    "    bad_channels= list(compress(raw_od.ch_names, sci < 0.8))\n",
    "    \n",
    "    if len(bad_channels) > 55:\n",
    "        print(f\"❌ Too many bad channels ({len(bad_channels)}). Excluding subject from analysis.\")\n",
    "        return None\n",
    "    \n",
    "    raw_od.info[\"bads\"] = bad_channels\n",
    "    raw_intensity_cropped.info[\"bads\"] = bad_channels\n",
    "    \n",
    "    print(f\"Bad channels: {raw_od.info['bads']}\")\n",
    "    # print the number of bad channels\n",
    "    print(f\"Number of bad channels: {len(raw_od.info['bads'])}\")\n",
    "    \n",
    "    # Remove bad channels\n",
    "   \n",
    "    \n",
    "    \n",
    "    raw_od = temporal_derivative_distribution_repair(raw_od)\n",
    "    raw_od_cropped = temporal_derivative_distribution_repair(raw_od_cropped)\n",
    "\n",
    "    \n",
    "     # Get long channels\n",
    "    long_chs = get_long_channels(raw_od)\n",
    "    bad_long_chs = long_chs.info[\"bads\"]\n",
    "    \n",
    "    \"\"\" print(f\"Number of all (good and bad) short channels: {tot_number_of_short_channels}\")\n",
    "    print(f\"Number of bad long channels: {len(bad_long_chs)}\")\n",
    "    print(f\"Number of long and short bad channels: {len(bad_channels)}\") \"\"\"\n",
    "    # print the number of short bad channels\n",
    "    len_bad_short_chs = len(bad_channels) - len(bad_long_chs)\n",
    "    #print(f\"Number of bad short channels: {len_bad_short_chs}\")\n",
    "    \n",
    "    # Determine if there are short channels\n",
    "    # print the number of short bad channels\n",
    "    len_bad_short_chs = len(bad_channels) - len(bad_long_chs)\n",
    "    num_good_short_channels = tot_number_of_short_channels - len_bad_short_chs\n",
    "    # Print diagnostics\n",
    "    print(f\"Number of all (good and bad) short channels: {tot_number_of_short_channels}\")\n",
    "    print(f\"Number of bad long channels: {len(bad_long_chs)}\")\n",
    "    print(f\"Number of bad short channels: {len_bad_short_chs}\")\n",
    "    print(f\"✅ Number of good short channels: {num_good_short_channels}\")\n",
    "    #print(f\"Number of bad short channels: {len_bad_short_chs}\")\n",
    "    \n",
    "    # Determine if there are short channels\n",
    "    if num_good_short_channels < 4:\n",
    "        print(\"❌ No short channels found. Skipping the subject.\")\n",
    "        return None, None, None, None, None # Keep the data unchanged\n",
    "    else:\n",
    "        #print(\"Applying short-channel regression.\")\n",
    "        raw_od_corrected = short_channel_regression(raw_od)\n",
    "        #raw_od_corrected=raw_od.copy()\n",
    "        # drop the bad channels\n",
    "        #raw_od_corrected.drop_channels(bad_channels)\n",
    "        \n",
    "        # interpolate the bad channels\n",
    "        #raw_od_corrected.interpolate_bads()\n",
    "        \n",
    "    # short-channel regression subtracts a scaled version of the signal obtained from the nearest short channel from the signal obtained from the long channel. \n",
    "\n",
    "    raw_haemo = beer_lambert_law(raw_od_corrected, ppf=0.1)\n",
    "    \n",
    "    #raw_haemo = get_long_channels(raw_haemo, min_dist=0.02, max_dist=0.04) # max_dist 40mm\n",
    "    raw_haemo = get_long_channels(raw_haemo, min_dist=0.02) \n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "\n",
    "    # Convert to haemoglobin and filter\n",
    "    \n",
    "    # check the ppf\n",
    "    \n",
    "    #raw_haemo.plot_psd(average=True, show=True)\n",
    "    \n",
    "    \"\"\" raw_haemo = raw_haemo.filter(\n",
    "    l_freq=0.01, h_freq=0.7, method=\"fir\", fir_design=\"firwin\", verbose=False,\n",
    "    h_trans_bandwidth=0.3, l_trans_bandwidth=0.005) \"\"\"\n",
    "    \n",
    "    # improved filter\n",
    "    \"\"\" raw_haemo = raw_haemo.filter(\n",
    "    l_freq=0.05, h_freq=0.2, method=\"fir\", fir_design=\"firwin\", verbose=False,\n",
    "    h_trans_bandwidth=0.01, l_trans_bandwidth=0.01) \"\"\"\n",
    "    \n",
    "    #raw_haemo.plot_psd(average=True, show=True)\n",
    "\n",
    "    raw_haemo = raw_haemo.filter(l_freq = None, h_freq = 0.2,  \n",
    "                                 method=\"iir\", iir_params =dict(order=5, ftype='butter'))\n",
    "     #high-pass\n",
    "    raw_haemo= raw_haemo.filter(l_freq =  0.05, h_freq = None, method=\"iir\", iir_params =dict(order=5, ftype='butter'))\n",
    "        \n",
    "    raw_stim = raw_haemo.copy()\n",
    "    #raw_stim.annotations.delete(raw_stim.annotations.description == 'Control') \n",
    "    \n",
    "    isis, names = mne_nirs.experimental_design.longest_inter_annotation_interval(raw_stim)\n",
    "    # Create a design matrix\n",
    "    design_matrix = make_first_level_design_matrix(raw_stim,\n",
    "                                                drift_model='cosine',\n",
    "                                                high_pass=1/(2*max(isis)),  # Must be specified per experiment\n",
    "                                                hrf_model='spm',\n",
    "                                                stim_dur=5.125)\n",
    "    \n",
    "    \n",
    "    # Run GLM\n",
    "    glm_est = run_glm(raw_haemo, design_matrix)\n",
    "\n",
    "    # Define ROI channel pairs\n",
    "    left = [[4, 2], [4, 3], [5, 2], [5, 3], [5, 4], [5, 5]]\n",
    "    right = [[10, 9], [10, 10], [10, 11], [10, 12], [11, 11], [11, 12]]\n",
    "    back = [[6, 6], [6, 8], [7, 6], [7, 7], [7, 8], [8, 7], [8, 8], [9, 8]]\n",
    "    front = [[1, 1], [2, 1], [3, 1], [3, 2], [12, 1]]\n",
    "    noise= [[10, 11], [5, 3], [11, 11], [10, 9], [10, 12], [10, 10]]\n",
    "    speech= [[10, 11], [10, 10], [10, 12], [11, 12], [1, 1], [10, 9], [8, 7], [7, 7], [8, 8]]\n",
    "    common= [[10, 11], [10, 9], [10, 12], [10, 10]]\n",
    "    only_noise= [[5, 3], [11, 11]]\n",
    "    only_speech= [ [11, 12], [1, 1], [10, 9], [8, 7], [7, 7], [8, 8]]\n",
    "    \n",
    "\n",
    "    # Generate index picks for each ROI\n",
    "    roi_picks = dict(\n",
    "        Left= picks_pair_to_idx(raw_haemo, left, on_missing=\"ignore\"),\n",
    "        Right= picks_pair_to_idx(raw_haemo, right, on_missing=\"ignore\"),\n",
    "        Back= picks_pair_to_idx(raw_haemo, back, on_missing=\"ignore\"),\n",
    "        Front= picks_pair_to_idx(raw_haemo, front, on_missing=\"ignore\"),\n",
    "        Noise= picks_pair_to_idx(raw_haemo, noise, on_missing=\"ignore\"),\n",
    "        Speech= picks_pair_to_idx(raw_haemo, speech, on_missing=\"ignore\"),\n",
    "        Common= picks_pair_to_idx(raw_haemo, common, on_missing=\"ignore\"),\n",
    "        OnlyNoise= picks_pair_to_idx(raw_haemo, only_noise, on_missing=\"ignore\"),\n",
    "        OnlySpeech= picks_pair_to_idx(raw_haemo, only_speech, on_missing=\"ignore\")\n",
    "    )\n",
    "    \n",
    "    cha= glm_est.to_dataframe()\n",
    "    \n",
    "    roi= glm_est.to_dataframe_region_of_interest(\n",
    "        roi_picks, design_matrix.columns, demographic_info=True\n",
    "    )\n",
    "    \n",
    "    # Define left vs right tapping contrast\n",
    "    contrast_matrix = np.eye(design_matrix.shape[1])\n",
    "    basic_conts = dict(\n",
    "        [(column, contrast_matrix[i]) for i, column in enumerate(design_matrix.columns)]\n",
    "    )\n",
    "    contrast_LvR = basic_conts[\"Noise\"] - basic_conts[\"Speech\"]\n",
    "\n",
    "    # Compute defined contrast\n",
    "    contrast = glm_est.compute_contrast(contrast_LvR)\n",
    "    con = contrast.to_dataframe()\n",
    "\n",
    "    # Add the participant sub to the dataframes\n",
    "    roi[\"ID\"] = cha[\"ID\"] = con[\"ID\"] = id\n",
    "    \n",
    "    roi[\"Subject\"] = cha[\"Subject\"] = con[\"Subject\"] = subject_id\n",
    "    \n",
    "    # Add the session to the dataframes\n",
    "    roi[\"session\"] = cha[\"session\"] = con[\"session\"] = session_id\n",
    "\n",
    "    # Convert to uM for nicer plotting below.\n",
    "    cha[\"theta\"] = [t * 1.0e6 for t in cha[\"theta\"]]\n",
    "    roi[\"theta\"] = [t * 1.0e6 for t in roi[\"theta\"]]\n",
    "    con[\"effect\"] = [t * 1.0e6 for t in con[\"effect\"]]\n",
    "    \n",
    "    \n",
    "    \n",
    "    return raw_haemo, roi, cha, con\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected subjects: ['01', '02', '03', '04', '05', '07', '08', '10', '11', '12', '13', '16', '17', '19', '21', '24']\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S4_D14 785', 'S4_D14 830', 'S5_D15 785', 'S5_D15 830', 'S6_D6 785', 'S6_D6 830', 'S6_D8 785', 'S6_D8 830', 'S7_D6 785', 'S7_D6 830', 'S7_D8 785', 'S7_D8 830', 'S8_D17 785', 'S8_D17 830', 'S9_D8 785', 'S9_D8 830']\n",
      "Number of bad channels: 16\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 10\n",
      "Number of bad short channels: 6\n",
      "✅ Number of good short channels: 10\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S2_D13 785', 'S2_D13 830', 'S7_D6 785', 'S7_D6 830', 'S7_D8 785', 'S7_D8 830', 'S8_D7 785', 'S8_D7 830', 'S8_D17 785', 'S8_D17 830', 'S9_D8 785', 'S9_D8 830', 'S10_D18 785', 'S10_D18 830', 'S12_D20 785', 'S12_D20 830']\n",
      "Number of bad channels: 16\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 8\n",
      "Number of bad short channels: 8\n",
      "✅ Number of good short channels: 8\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S4_D14 785', 'S4_D14 830', 'S5_D15 785', 'S5_D15 830', 'S7_D6 785', 'S7_D6 830', 'S8_D17 785', 'S8_D17 830']\n",
      "Number of bad channels: 8\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 2\n",
      "Number of bad short channels: 6\n",
      "✅ Number of good short channels: 10\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S5_D15 785', 'S5_D15 830', 'S6_D6 785', 'S6_D6 830', 'S6_D8 785', 'S6_D8 830', 'S6_D16 785', 'S6_D16 830', 'S7_D6 785', 'S7_D6 830', 'S8_D7 785', 'S8_D7 830', 'S8_D8 785', 'S8_D8 830', 'S8_D17 785', 'S8_D17 830', 'S10_D18 785', 'S10_D18 830', 'S11_D19 785', 'S11_D19 830']\n",
      "Number of bad channels: 20\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 10\n",
      "Number of bad short channels: 10\n",
      "✅ Number of good short channels: 6\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S5_D15 785', 'S5_D15 830', 'S6_D16 785', 'S6_D16 830', 'S7_D6 785', 'S7_D6 830', 'S10_D18 785', 'S10_D18 830', 'S11_D19 785', 'S11_D19 830']\n",
      "Number of bad channels: 10\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 2\n",
      "Number of bad short channels: 8\n",
      "✅ Number of good short channels: 8\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S2_D13 785', 'S2_D13 830', 'S5_D15 785', 'S5_D15 830', 'S8_D8 785', 'S8_D8 830', 'S8_D17 785', 'S8_D17 830', 'S10_D18 785', 'S10_D18 830', 'S12_D20 785', 'S12_D20 830']\n",
      "Number of bad channels: 12\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 2\n",
      "Number of bad short channels: 10\n",
      "✅ Number of good short channels: 6\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S5_D15 785', 'S5_D15 830', 'S6_D16 785', 'S6_D16 830', 'S8_D17 785', 'S8_D17 830', 'S10_D18 785', 'S10_D18 830', 'S11_D19 785', 'S11_D19 830']\n",
      "Number of bad channels: 10\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 0\n",
      "Number of bad short channels: 10\n",
      "✅ Number of good short channels: 6\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S4_D14 785', 'S4_D14 830', 'S6_D16 785', 'S6_D16 830', 'S8_D17 785', 'S8_D17 830', 'S10_D18 785', 'S10_D18 830']\n",
      "Number of bad channels: 8\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 0\n",
      "Number of bad short channels: 8\n",
      "✅ Number of good short channels: 8\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S2_D13 785', 'S2_D13 830', 'S4_D14 785', 'S4_D14 830', 'S5_D15 785', 'S5_D15 830', 'S6_D6 785', 'S6_D6 830', 'S6_D16 785', 'S6_D16 830', 'S7_D6 785', 'S7_D6 830']\n",
      "Number of bad channels: 12\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 4\n",
      "Number of bad short channels: 8\n",
      "✅ Number of good short channels: 8\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S2_D13 785', 'S2_D13 830', 'S4_D14 785', 'S4_D14 830', 'S5_D2 785', 'S5_D2 830', 'S5_D3 785', 'S5_D3 830', 'S5_D4 785', 'S5_D4 830', 'S5_D5 785', 'S5_D5 830', 'S5_D15 785', 'S5_D15 830', 'S7_D8 785', 'S7_D8 830', 'S10_D18 785', 'S10_D18 830', 'S12_D20 785', 'S12_D20 830']\n",
      "Number of bad channels: 20\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 10\n",
      "Number of bad short channels: 10\n",
      "✅ Number of good short channels: 6\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S2_D13 785', 'S2_D13 830', 'S4_D14 785', 'S4_D14 830', 'S5_D15 785', 'S5_D15 830', 'S6_D16 785', 'S6_D16 830', 'S12_D20 785', 'S12_D20 830']\n",
      "Number of bad channels: 10\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 0\n",
      "Number of bad short channels: 10\n",
      "✅ Number of good short channels: 6\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S2_D13 785', 'S2_D13 830', 'S4_D14 785', 'S4_D14 830', 'S5_D15 785', 'S5_D15 830', 'S6_D16 785', 'S6_D16 830', 'S12_D20 785', 'S12_D20 830']\n",
      "Number of bad channels: 10\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 0\n",
      "Number of bad short channels: 10\n",
      "✅ Number of good short channels: 6\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S6_D16 785', 'S6_D16 830', 'S10_D18 785', 'S10_D18 830', 'S11_D19 785', 'S11_D19 830']\n",
      "Number of bad channels: 6\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 0\n",
      "Number of bad short channels: 6\n",
      "✅ Number of good short channels: 10\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S6_D16 785', 'S6_D16 830', 'S10_D18 785', 'S10_D18 830', 'S11_D19 785', 'S11_D19 830']\n",
      "Number of bad channels: 6\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 0\n",
      "Number of bad short channels: 6\n",
      "✅ Number of good short channels: 10\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S2_D13 785', 'S2_D13 830', 'S4_D14 785', 'S4_D14 830', 'S5_D15 785', 'S5_D15 830', 'S6_D6 785', 'S6_D6 830']\n",
      "Number of bad channels: 8\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 2\n",
      "Number of bad short channels: 6\n",
      "✅ Number of good short channels: 10\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S2_D13 785', 'S2_D13 830', 'S4_D14 785', 'S4_D14 830', 'S5_D15 785', 'S5_D15 830', 'S6_D6 785', 'S6_D6 830', 'S11_D19 785', 'S11_D19 830']\n",
      "Number of bad channels: 10\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 2\n",
      "Number of bad short channels: 8\n",
      "✅ Number of good short channels: 8\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S6_D6 785', 'S6_D6 830', 'S6_D16 785', 'S6_D16 830', 'S7_D6 785', 'S7_D6 830', 'S7_D7 785', 'S7_D7 830', 'S7_D8 785', 'S7_D8 830', 'S8_D7 785', 'S8_D7 830', 'S8_D17 785', 'S8_D17 830', 'S10_D18 785', 'S10_D18 830', 'S11_D19 785', 'S11_D19 830']\n",
      "Number of bad channels: 18\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 10\n",
      "Number of bad short channels: 8\n",
      "✅ Number of good short channels: 8\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S5_D15 785', 'S5_D15 830', 'S6_D6 785', 'S6_D6 830', 'S6_D16 785', 'S6_D16 830', 'S7_D6 785', 'S7_D6 830', 'S7_D7 785', 'S7_D7 830', 'S7_D8 785', 'S7_D8 830', 'S8_D17 785', 'S8_D17 830', 'S10_D18 785', 'S10_D18 830', 'S11_D19 785', 'S11_D19 830']\n",
      "Number of bad channels: 18\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 8\n",
      "Number of bad short channels: 10\n",
      "✅ Number of good short channels: 6\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S2_D13 785', 'S2_D13 830', 'S4_D14 785', 'S4_D14 830', 'S11_D19 785', 'S11_D19 830', 'S12_D20 785', 'S12_D20 830']\n",
      "Number of bad channels: 8\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 0\n",
      "Number of bad short channels: 8\n",
      "✅ Number of good short channels: 8\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S2_D13 785', 'S2_D13 830', 'S4_D14 785', 'S4_D14 830', 'S11_D19 785', 'S11_D19 830', 'S12_D20 785', 'S12_D20 830']\n",
      "Number of bad channels: 8\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 0\n",
      "Number of bad short channels: 8\n",
      "✅ Number of good short channels: 8\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S2_D13 785', 'S2_D13 830', 'S4_D14 785', 'S4_D14 830', 'S5_D15 785', 'S5_D15 830', 'S6_D16 785', 'S6_D16 830', 'S7_D8 785', 'S7_D8 830', 'S10_D12 785', 'S10_D12 830', 'S12_D20 785', 'S12_D20 830']\n",
      "Number of bad channels: 14\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 4\n",
      "Number of bad short channels: 10\n",
      "✅ Number of good short channels: 6\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S4_D14 785', 'S4_D14 830', 'S6_D16 785', 'S6_D16 830', 'S7_D8 785', 'S7_D8 830', 'S9_D8 785', 'S9_D8 830', 'S11_D19 785', 'S11_D19 830', 'S12_D20 785', 'S12_D20 830']\n",
      "Number of bad channels: 12\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 4\n",
      "Number of bad short channels: 8\n",
      "✅ Number of good short channels: 8\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S6_D16 785', 'S6_D16 830', 'S10_D18 785', 'S10_D18 830']\n",
      "Number of bad channels: 4\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 0\n",
      "Number of bad short channels: 4\n",
      "✅ Number of good short channels: 12\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S5_D3 785', 'S5_D3 830', 'S5_D15 785', 'S5_D15 830', 'S6_D16 785', 'S6_D16 830', 'S8_D8 785', 'S8_D8 830', 'S8_D17 785', 'S8_D17 830', 'S10_D18 785', 'S10_D18 830']\n",
      "Number of bad channels: 12\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 4\n",
      "Number of bad short channels: 8\n",
      "✅ Number of good short channels: 8\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S5_D15 785', 'S5_D15 830', 'S6_D16 785', 'S6_D16 830', 'S7_D8 785', 'S7_D8 830', 'S11_D19 785', 'S11_D19 830']\n",
      "Number of bad channels: 8\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 2\n",
      "Number of bad short channels: 6\n",
      "✅ Number of good short channels: 10\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S5_D15 785', 'S5_D15 830', 'S6_D16 785', 'S6_D16 830', 'S7_D6 785', 'S7_D6 830', 'S7_D8 785', 'S7_D8 830', 'S9_D8 785', 'S9_D8 830', 'S10_D18 785', 'S10_D18 830']\n",
      "Number of bad channels: 12\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 6\n",
      "Number of bad short channels: 6\n",
      "✅ Number of good short channels: 10\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S4_D14 785', 'S4_D14 830', 'S5_D15 785', 'S5_D15 830', 'S7_D8 785', 'S7_D8 830', 'S11_D19 785', 'S11_D19 830']\n",
      "Number of bad channels: 8\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 2\n",
      "Number of bad short channels: 6\n",
      "✅ Number of good short channels: 10\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S6_D16 785', 'S6_D16 830', 'S7_D8 785', 'S7_D8 830', 'S10_D18 785', 'S10_D18 830']\n",
      "Number of bad channels: 6\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 2\n",
      "Number of bad short channels: 4\n",
      "✅ Number of good short channels: 12\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S4_D14 785', 'S4_D14 830', 'S5_D15 785', 'S5_D15 830', 'S6_D16 785', 'S6_D16 830', 'S7_D8 785', 'S7_D8 830', 'S8_D17 785', 'S8_D17 830', 'S11_D19 785', 'S11_D19 830']\n",
      "Number of bad channels: 12\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 2\n",
      "Number of bad short channels: 10\n",
      "✅ Number of good short channels: 6\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S2_D13 785', 'S2_D13 830', 'S4_D14 785', 'S4_D14 830', 'S5_D15 785', 'S5_D15 830', 'S6_D16 785', 'S6_D16 830', 'S8_D7 785', 'S8_D7 830', 'S8_D8 785', 'S8_D8 830', 'S8_D17 785', 'S8_D17 830', 'S10_D9 785', 'S10_D9 830', 'S12_D20 785', 'S12_D20 830']\n",
      "Number of bad channels: 18\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 6\n",
      "Number of bad short channels: 12\n",
      "✅ Number of good short channels: 4\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S4_D14 785', 'S4_D14 830', 'S5_D3 785', 'S5_D3 830', 'S5_D5 785', 'S5_D5 830', 'S5_D15 785', 'S5_D15 830', 'S6_D16 785', 'S6_D16 830', 'S7_D6 785', 'S7_D6 830', 'S7_D7 785', 'S7_D7 830', 'S7_D8 785', 'S7_D8 830', 'S8_D17 785', 'S8_D17 830', 'S9_D8 785', 'S9_D8 830', 'S10_D18 785', 'S10_D18 830', 'S12_D20 785', 'S12_D20 830']\n",
      "Number of bad channels: 24\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 12\n",
      "Number of bad short channels: 12\n",
      "✅ Number of good short channels: 4\n",
      "Removing break annotations for the orginal raw...\n",
      "Removing break annotations for the cropped raw...\n",
      "Bad channels: ['S2_D13 785', 'S2_D13 830', 'S5_D15 785', 'S5_D15 830', 'S6_D6 785', 'S6_D6 830', 'S6_D16 785', 'S6_D16 830', 'S7_D6 785', 'S7_D6 830', 'S7_D7 785', 'S7_D7 830', 'S7_D8 785', 'S7_D8 830', 'S8_D7 785', 'S8_D7 830', 'S8_D17 785', 'S8_D17 830', 'S10_D18 785', 'S10_D18 830', 'S12_D20 785', 'S12_D20 830']\n",
      "Number of bad channels: 22\n",
      "Number of all (good and bad) short channels: 16\n",
      "Number of bad long channels: 10\n",
      "Number of bad short channels: 12\n",
      "✅ Number of good short channels: 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bids_root = r\"C:\\Datasets\\Test-retest study\\bids_dataset\"\n",
    "subject_list = sorted([d for d in os.listdir(bids_root) if d.startswith(\"sub-\")])\n",
    "subject_list = [s.replace(\"sub-\", \"\") for s in subject_list]\n",
    "#subject_list = subject_list[:1]  # Limit to first 3 subjects for testing\n",
    "\n",
    "print(\"Detected subjects:\", subject_list)\n",
    "subjects_df = pd.DataFrame(columns=[\"subject\", \"session\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#optode_subject_sessions = {optode: pd.DataFrame(columns=[\"subject\", \"session\"]) for optode in optodes}\n",
    "\n",
    "df_roi = pd.DataFrame()  # To store region of interest results\n",
    "df_cha = pd.DataFrame()  # To store channel level results\n",
    "df_con = pd.DataFrame()  # To store channel level contrast results\n",
    "id = 0\n",
    "# Loop through subjects and sessions\n",
    "for sub in subject_list:\n",
    "    for ses in range(1, 3):\n",
    "        \n",
    "        bids_path = BIDSPath(\n",
    "                subject=f\"{sub}\",\n",
    "                session=f\"{ses:02d}\",\n",
    "                task=\"auditory\",\n",
    "                datatype=\"nirs\",\n",
    "                root=bids_root,\n",
    "                suffix=\"nirs\",\n",
    "                extension=\".snirf\",\n",
    "            )\n",
    "            \n",
    "        raw_haemo, roi, cha, con = preprocessing_glm_ROI(bids_path, sub, ses, id)\n",
    "        if raw_haemo is None:\n",
    "            print(f\"⚠️ No data for Subject {sub}, Session {ses:02d}. Skipping...\")\n",
    "            continue\n",
    "        else:\n",
    "            \n",
    "            # Append individual results to all participants\n",
    "            df_roi = pd.concat([df_roi, roi], ignore_index=True)\n",
    "            df_cha = pd.concat([df_cha, cha], ignore_index=True)\n",
    "            df_con = pd.concat([df_con, con], ignore_index=True)\n",
    "            id= id + 1\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model for channel S1_D1 hbo...\n",
      "Fitting model for channel S2_D1 hbo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarab\\anaconda3\\envs\\mne\\Lib\\site-packages\\statsmodels\\regression\\mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "c:\\Users\\sarab\\anaconda3\\envs\\mne\\Lib\\site-packages\\statsmodels\\regression\\mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model for channel S3_D1 hbo...\n",
      "Fitting model for channel S3_D2 hbo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarab\\anaconda3\\envs\\mne\\Lib\\site-packages\\statsmodels\\regression\\mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model for channel S4_D2 hbo...\n",
      "Fitting model for channel S4_D3 hbo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarab\\anaconda3\\envs\\mne\\Lib\\site-packages\\statsmodels\\regression\\mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model for channel S5_D2 hbo...\n",
      "Fitting model for channel S5_D3 hbo...\n",
      "Fitting model for channel S5_D4 hbo...\n",
      "Fitting model for channel S5_D5 hbo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarab\\anaconda3\\envs\\mne\\Lib\\site-packages\\statsmodels\\regression\\mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "c:\\Users\\sarab\\anaconda3\\envs\\mne\\Lib\\site-packages\\statsmodels\\regression\\mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model for channel S6_D6 hbo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarab\\anaconda3\\envs\\mne\\Lib\\site-packages\\statsmodels\\regression\\mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "c:\\Users\\sarab\\anaconda3\\envs\\mne\\Lib\\site-packages\\statsmodels\\regression\\mixed_linear_model.py:2261: ConvergenceWarning: The Hessian matrix at the estimated parameter values is not positive definite.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "c:\\Users\\sarab\\anaconda3\\envs\\mne\\Lib\\site-packages\\statsmodels\\regression\\mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model for channel S6_D8 hbo...\n",
      "Fitting model for channel S7_D6 hbo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarab\\anaconda3\\envs\\mne\\Lib\\site-packages\\statsmodels\\regression\\mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "c:\\Users\\sarab\\anaconda3\\envs\\mne\\Lib\\site-packages\\statsmodels\\regression\\mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model for channel S7_D7 hbo...\n",
      "Fitting model for channel S7_D8 hbo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarab\\anaconda3\\envs\\mne\\Lib\\site-packages\\statsmodels\\regression\\mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "c:\\Users\\sarab\\anaconda3\\envs\\mne\\Lib\\site-packages\\statsmodels\\regression\\mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model for channel S8_D7 hbo...\n",
      "Fitting model for channel S8_D8 hbo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarab\\anaconda3\\envs\\mne\\Lib\\site-packages\\statsmodels\\regression\\mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "c:\\Users\\sarab\\anaconda3\\envs\\mne\\Lib\\site-packages\\statsmodels\\regression\\mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model for channel S9_D8 hbo...\n",
      "Fitting model for channel S10_D9 hbo...\n",
      "Fitting model for channel S10_D10 hbo...\n",
      "Fitting model for channel S10_D11 hbo...\n",
      "Fitting model for channel S10_D12 hbo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarab\\anaconda3\\envs\\mne\\Lib\\site-packages\\statsmodels\\regression\\mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n",
      "c:\\Users\\sarab\\anaconda3\\envs\\mne\\Lib\\site-packages\\statsmodels\\regression\\mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model for channel S11_D11 hbo...\n",
      "Fitting model for channel S11_D12 hbo...\n",
      "Fitting model for channel S12_D1 hbo...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarab\\anaconda3\\envs\\mne\\Lib\\site-packages\\statsmodels\\regression\\mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Channel",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Comparison",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Beta",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "StdErr",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "z",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "p",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "622c36a7-347a-4f05-b3b7-771d6da48736",
       "rows": [
        [
         "2",
         "S2_D1 hbo",
         "Condition[T.Noise]",
         "-1.3335616115846731",
         "0.6775126226461954",
         "-1.9683199500787365",
         "0.049031235908921035"
        ],
        [
         "3",
         "S2_D1 hbo",
         "Condition[T.Speech]",
         "-1.8672651724255491",
         "0.6775126226461954",
         "-2.7560596068785803",
         "0.005850233467918856"
        ],
        [
         "7",
         "S3_D2 hbo",
         "Condition[T.Speech]",
         "-2.4295192778975507",
         "0.9313555720065169",
         "-2.6085840369896354",
         "0.009091768454306525"
        ],
        [
         "13",
         "S5_D2 hbo",
         "Condition[T.Speech]",
         "-2.0196032734604286",
         "0.9669729016427422",
         "-2.08858311337311",
         "0.036745266581471384"
        ],
        [
         "22",
         "S6_D8 hbo",
         "Condition[T.Noise]",
         "-1.4742185679684277",
         "0.6298659876069989",
         "-2.3405273454585354",
         "0.01925652817724055"
        ],
        [
         "23",
         "S6_D8 hbo",
         "Condition[T.Speech]",
         "-2.2448632249942717",
         "0.6298659876069989",
         "-3.5640330946000227",
         "0.0003651998799990123"
        ],
        [
         "25",
         "S7_D6 hbo",
         "Condition[T.Speech]",
         "-3.6071118140275096",
         "0.9070981203433335",
         "-3.976539839661701",
         "6.992528857469502e-05"
        ],
        [
         "27",
         "S7_D7 hbo",
         "Condition[T.Speech]",
         "-2.0798527726187666",
         "0.7556155651681445",
         "-2.7525276986002054",
         "0.0059137143569507325"
        ],
        [
         "29",
         "S7_D8 hbo",
         "Condition[T.Speech]",
         "-3.9307842306051484",
         "1.3069144451847565",
         "-3.007682901576208",
         "0.0026324766123379927"
        ],
        [
         "31",
         "S8_D7 hbo",
         "Condition[T.Speech]",
         "-2.6528260945298054",
         "1.0643484492750184",
         "-2.4924413582194624",
         "0.012686827839827348"
        ],
        [
         "33",
         "S8_D8 hbo",
         "Condition[T.Speech]",
         "-2.433449941883264",
         "1.139298336480857",
         "-2.135919858708713",
         "0.03268594663084335"
        ],
        [
         "35",
         "S9_D8 hbo",
         "Condition[T.Speech]",
         "-2.816944223346658",
         "0.9100999123775175",
         "-3.0952032683837514",
         "0.001966780584304996"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 12
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Channel</th>\n",
       "      <th>Comparison</th>\n",
       "      <th>Beta</th>\n",
       "      <th>StdErr</th>\n",
       "      <th>z</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S2_D1 hbo</td>\n",
       "      <td>Condition[T.Noise]</td>\n",
       "      <td>-1.333562</td>\n",
       "      <td>0.677513</td>\n",
       "      <td>-1.968320</td>\n",
       "      <td>0.049031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S2_D1 hbo</td>\n",
       "      <td>Condition[T.Speech]</td>\n",
       "      <td>-1.867265</td>\n",
       "      <td>0.677513</td>\n",
       "      <td>-2.756060</td>\n",
       "      <td>0.005850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>S3_D2 hbo</td>\n",
       "      <td>Condition[T.Speech]</td>\n",
       "      <td>-2.429519</td>\n",
       "      <td>0.931356</td>\n",
       "      <td>-2.608584</td>\n",
       "      <td>0.009092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>S5_D2 hbo</td>\n",
       "      <td>Condition[T.Speech]</td>\n",
       "      <td>-2.019603</td>\n",
       "      <td>0.966973</td>\n",
       "      <td>-2.088583</td>\n",
       "      <td>0.036745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>S6_D8 hbo</td>\n",
       "      <td>Condition[T.Noise]</td>\n",
       "      <td>-1.474219</td>\n",
       "      <td>0.629866</td>\n",
       "      <td>-2.340527</td>\n",
       "      <td>0.019257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>S6_D8 hbo</td>\n",
       "      <td>Condition[T.Speech]</td>\n",
       "      <td>-2.244863</td>\n",
       "      <td>0.629866</td>\n",
       "      <td>-3.564033</td>\n",
       "      <td>0.000365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>S7_D6 hbo</td>\n",
       "      <td>Condition[T.Speech]</td>\n",
       "      <td>-3.607112</td>\n",
       "      <td>0.907098</td>\n",
       "      <td>-3.976540</td>\n",
       "      <td>0.000070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>S7_D7 hbo</td>\n",
       "      <td>Condition[T.Speech]</td>\n",
       "      <td>-2.079853</td>\n",
       "      <td>0.755616</td>\n",
       "      <td>-2.752528</td>\n",
       "      <td>0.005914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>S7_D8 hbo</td>\n",
       "      <td>Condition[T.Speech]</td>\n",
       "      <td>-3.930784</td>\n",
       "      <td>1.306914</td>\n",
       "      <td>-3.007683</td>\n",
       "      <td>0.002632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>S8_D7 hbo</td>\n",
       "      <td>Condition[T.Speech]</td>\n",
       "      <td>-2.652826</td>\n",
       "      <td>1.064348</td>\n",
       "      <td>-2.492441</td>\n",
       "      <td>0.012687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>S8_D8 hbo</td>\n",
       "      <td>Condition[T.Speech]</td>\n",
       "      <td>-2.433450</td>\n",
       "      <td>1.139298</td>\n",
       "      <td>-2.135920</td>\n",
       "      <td>0.032686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>S9_D8 hbo</td>\n",
       "      <td>Condition[T.Speech]</td>\n",
       "      <td>-2.816944</td>\n",
       "      <td>0.910100</td>\n",
       "      <td>-3.095203</td>\n",
       "      <td>0.001967</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Channel           Comparison      Beta    StdErr         z         p\n",
       "2   S2_D1 hbo   Condition[T.Noise] -1.333562  0.677513 -1.968320  0.049031\n",
       "3   S2_D1 hbo  Condition[T.Speech] -1.867265  0.677513 -2.756060  0.005850\n",
       "7   S3_D2 hbo  Condition[T.Speech] -2.429519  0.931356 -2.608584  0.009092\n",
       "13  S5_D2 hbo  Condition[T.Speech] -2.019603  0.966973 -2.088583  0.036745\n",
       "22  S6_D8 hbo   Condition[T.Noise] -1.474219  0.629866 -2.340527  0.019257\n",
       "23  S6_D8 hbo  Condition[T.Speech] -2.244863  0.629866 -3.564033  0.000365\n",
       "25  S7_D6 hbo  Condition[T.Speech] -3.607112  0.907098 -3.976540  0.000070\n",
       "27  S7_D7 hbo  Condition[T.Speech] -2.079853  0.755616 -2.752528  0.005914\n",
       "29  S7_D8 hbo  Condition[T.Speech] -3.930784  1.306914 -3.007683  0.002632\n",
       "31  S8_D7 hbo  Condition[T.Speech] -2.652826  1.064348 -2.492441  0.012687\n",
       "33  S8_D8 hbo  Condition[T.Speech] -2.433450  1.139298 -2.135920  0.032686\n",
       "35  S9_D8 hbo  Condition[T.Speech] -2.816944  0.910100 -3.095203  0.001967"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Step 1: Filter for session 1 and hbo only\n",
    "ch_summary = df_cha.query(\"Condition in ['Control', 'Noise', 'Speech'] and Chroma == 'hbo' and session == 2\").copy()\n",
    "\n",
    "# Step 2: Ensure 'Condition' is categorical with 'Control' as the reference\n",
    "ch_summary[\"Condition\"] = pd.Categorical(\n",
    "    ch_summary[\"Condition\"], categories=[\"Control\", \"Noise\", \"Speech\"], ordered=True\n",
    ")\n",
    "\n",
    "# Step 3: Fit per-channel models and collect results\n",
    "results = []\n",
    "\n",
    "for ch in ch_summary[\"ch_name\"].unique():\n",
    "    print(f\"Fitting model for channel {ch}...\")\n",
    "    ch_data = ch_summary[ch_summary[\"ch_name\"] == ch].copy()\n",
    "\n",
    "    try:\n",
    "        model = smf.mixedlm(\n",
    "            \"theta ~ Condition\",  # Compare Noise & Speech vs Control\n",
    "            ch_data,\n",
    "            groups=ch_data[\"Subject\"]\n",
    "        ).fit(method=\"nm\")\n",
    "\n",
    "        # Extract parameters and p-values directly\n",
    "        for param_name in model.params.index:\n",
    "            if \"Condition[T.Noise]\" in param_name or \"Condition[T.Speech]\" in param_name:\n",
    "                results.append({\n",
    "                    \"Channel\": ch,\n",
    "                    \"Comparison\": param_name,\n",
    "                    \"Beta\": model.params[param_name],\n",
    "                    \"StdErr\": model.bse[param_name],\n",
    "                    \"z\": model.tvalues[param_name],\n",
    "                    \"p\": model.pvalues[param_name]\n",
    "                })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Model failed for channel {ch}: {e}\")\n",
    "\n",
    "# Step 4: Create DataFrame from results\n",
    "significant = pd.DataFrame(results)\n",
    "\n",
    "# Step 5: Filter for significance\n",
    "significant = significant[significant[\"p\"] < 0.05]\n",
    "\n",
    "# View significant results\n",
    "significant\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LME MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running mixed model for Left...\n",
      "                   Mixed Linear Model Regression Results\n",
      "===========================================================================\n",
      "Model:                    MixedLM       Dependent Variable:       theta    \n",
      "No. Observations:         96            Method:                   REML     \n",
      "No. Groups:               16            Scale:                    4.1440   \n",
      "Min. group size:          6             Log-Likelihood:           -206.3042\n",
      "Max. group size:          6             Converged:                Yes      \n",
      "Mean group size:          6.0                                              \n",
      "---------------------------------------------------------------------------\n",
      "                                 Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "---------------------------------------------------------------------------\n",
      "Intercept                        -0.278    0.562 -0.494 0.621 -1.379  0.824\n",
      "Condition[T.Noise]               -0.716    0.720 -0.994 0.320 -2.126  0.695\n",
      "Condition[T.Speech]               0.396    0.720  0.550 0.583 -1.015  1.806\n",
      "session[T.2]                      0.794    0.720  1.103 0.270 -0.617  2.204\n",
      "Condition[T.Noise]:session[T.2]   0.364    1.018  0.358 0.720 -1.630  2.359\n",
      "Condition[T.Speech]:session[T.2] -0.786    1.018 -0.772 0.440 -2.781  1.209\n",
      "Group Var                         0.911    0.315                           \n",
      "===========================================================================\n",
      "\n",
      "Running mixed model for Right...\n",
      "                   Mixed Linear Model Regression Results\n",
      "===========================================================================\n",
      "Model:                    MixedLM       Dependent Variable:       theta    \n",
      "No. Observations:         96            Method:                   REML     \n",
      "No. Groups:               16            Scale:                    2.7892   \n",
      "Min. group size:          6             Log-Likelihood:           -192.5683\n",
      "Max. group size:          6             Converged:                Yes      \n",
      "Mean group size:          6.0                                              \n",
      "---------------------------------------------------------------------------\n",
      "                                 Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "---------------------------------------------------------------------------\n",
      "Intercept                        -0.023    0.511 -0.045 0.964 -1.025  0.979\n",
      "Condition[T.Noise]                0.644    0.590  1.091 0.275 -0.513  1.801\n",
      "Condition[T.Speech]               0.230    0.590  0.390 0.697 -0.927  1.388\n",
      "session[T.2]                      0.340    0.590  0.576 0.565 -0.817  1.497\n",
      "Condition[T.Noise]:session[T.2]  -0.075    0.835 -0.090 0.929 -1.711  1.562\n",
      "Condition[T.Speech]:session[T.2] -0.239    0.835 -0.286 0.775 -1.876  1.398\n",
      "Group Var                         1.392    0.445                           \n",
      "===========================================================================\n",
      "\n",
      "Running mixed model for Back...\n",
      "                   Mixed Linear Model Regression Results\n",
      "===========================================================================\n",
      "Model:                    MixedLM       Dependent Variable:       theta    \n",
      "No. Observations:         96            Method:                   REML     \n",
      "No. Groups:               16            Scale:                    3.3756   \n",
      "Min. group size:          6             Log-Likelihood:           -196.9882\n",
      "Max. group size:          6             Converged:                Yes      \n",
      "Mean group size:          6.0                                              \n",
      "---------------------------------------------------------------------------\n",
      "                                 Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "---------------------------------------------------------------------------\n",
      "Intercept                        -0.384    0.506 -0.757 0.449 -1.376  0.609\n",
      "Condition[T.Noise]               -0.893    0.650 -1.374 0.169 -2.166  0.380\n",
      "Condition[T.Speech]              -2.095    0.650 -3.225 0.001 -3.368 -0.822\n",
      "session[T.2]                      0.804    0.650  1.238 0.216 -0.469  2.077\n",
      "Condition[T.Noise]:session[T.2]   0.014    0.919  0.015 0.988 -1.787  1.814\n",
      "Condition[T.Speech]:session[T.2] -0.214    0.919 -0.233 0.816 -2.014  1.587\n",
      "Group Var                         0.727    0.281                           \n",
      "===========================================================================\n",
      "\n",
      "Running mixed model for Front...\n",
      "                   Mixed Linear Model Regression Results\n",
      "===========================================================================\n",
      "Model:                    MixedLM       Dependent Variable:       theta    \n",
      "No. Observations:         96            Method:                   REML     \n",
      "No. Groups:               16            Scale:                    3.0627   \n",
      "Min. group size:          6             Log-Likelihood:           -186.3904\n",
      "Max. group size:          6             Converged:                Yes      \n",
      "Mean group size:          6.0                                              \n",
      "---------------------------------------------------------------------------\n",
      "                                 Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "---------------------------------------------------------------------------\n",
      "Intercept                        -0.126    0.438 -0.287 0.774 -0.983  0.732\n",
      "Condition[T.Noise]               -0.546    0.619 -0.883 0.377 -1.759  0.666\n",
      "Condition[T.Speech]              -0.433    0.619 -0.700 0.484 -1.646  0.779\n",
      "session[T.2]                      0.923    0.619  1.492 0.136 -0.289  2.136\n",
      "Condition[T.Noise]:session[T.2]  -0.284    0.875 -0.325 0.745 -1.999  1.431\n",
      "Condition[T.Speech]:session[T.2] -1.142    0.875 -1.305 0.192 -2.857  0.573\n",
      "Group Var                         0.000    0.118                           \n",
      "===========================================================================\n",
      "\n",
      "Running mixed model for Noise...\n",
      "                   Mixed Linear Model Regression Results\n",
      "===========================================================================\n",
      "Model:                    MixedLM       Dependent Variable:       theta    \n",
      "No. Observations:         96            Method:                   REML     \n",
      "No. Groups:               16            Scale:                    3.0649   \n",
      "Min. group size:          6             Log-Likelihood:           -196.7407\n",
      "Max. group size:          6             Converged:                Yes      \n",
      "Mean group size:          6.0                                              \n",
      "---------------------------------------------------------------------------\n",
      "                                 Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "---------------------------------------------------------------------------\n",
      "Intercept                         0.065    0.535  0.122 0.903 -0.983  1.113\n",
      "Condition[T.Noise]                0.385    0.619  0.622 0.534 -0.828  1.598\n",
      "Condition[T.Speech]               0.133    0.619  0.216 0.829 -1.080  1.347\n",
      "session[T.2]                      0.136    0.619  0.219 0.826 -1.077  1.349\n",
      "Condition[T.Noise]:session[T.2]   0.111    0.875  0.127 0.899 -1.605  1.827\n",
      "Condition[T.Speech]:session[T.2] -0.103    0.875 -0.118 0.906 -1.818  1.613\n",
      "Group Var                         1.511    0.462                           \n",
      "===========================================================================\n",
      "\n",
      "Running mixed model for Speech...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sarab\\anaconda3\\envs\\mne\\Lib\\site-packages\\statsmodels\\regression\\mixed_linear_model.py:2237: ConvergenceWarning: The MLE may be on the boundary of the parameter space.\n",
      "  warnings.warn(msg, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Mixed Linear Model Regression Results\n",
      "===========================================================================\n",
      "Model:                    MixedLM       Dependent Variable:       theta    \n",
      "No. Observations:         96            Method:                   REML     \n",
      "No. Groups:               16            Scale:                    2.6677   \n",
      "Min. group size:          6             Log-Likelihood:           -189.2215\n",
      "Max. group size:          6             Converged:                Yes      \n",
      "Mean group size:          6.0                                              \n",
      "---------------------------------------------------------------------------\n",
      "                                 Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "---------------------------------------------------------------------------\n",
      "Intercept                        -0.182    0.481 -0.378 0.705 -1.126  0.762\n",
      "Condition[T.Noise]                0.217    0.577  0.376 0.707 -0.915  1.349\n",
      "Condition[T.Speech]              -0.562    0.577 -0.974 0.330 -1.694  0.570\n",
      "session[T.2]                      0.437    0.577  0.756 0.449 -0.695  1.569\n",
      "Condition[T.Noise]:session[T.2]  -0.111    0.817 -0.135 0.892 -1.711  1.490\n",
      "Condition[T.Speech]:session[T.2] -0.216    0.817 -0.265 0.791 -1.817  1.384\n",
      "Group Var                         1.040    0.364                           \n",
      "===========================================================================\n",
      "\n",
      "Running mixed model for Common...\n",
      "                   Mixed Linear Model Regression Results\n",
      "===========================================================================\n",
      "Model:                    MixedLM       Dependent Variable:       theta    \n",
      "No. Observations:         96            Method:                   REML     \n",
      "No. Groups:               16            Scale:                    4.1720   \n",
      "Min. group size:          6             Log-Likelihood:           -209.4887\n",
      "Max. group size:          6             Converged:                Yes      \n",
      "Mean group size:          6.0                                              \n",
      "---------------------------------------------------------------------------\n",
      "                                 Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "---------------------------------------------------------------------------\n",
      "Intercept                         0.275    0.604  0.455 0.649 -0.909  1.460\n",
      "Condition[T.Noise]                0.536    0.722  0.742 0.458 -0.880  1.951\n",
      "Condition[T.Speech]              -0.789    0.722 -1.092 0.275 -2.204  0.627\n",
      "session[T.2]                     -0.325    0.722 -0.451 0.652 -1.741  1.090\n",
      "Condition[T.Noise]:session[T.2]   0.227    1.021  0.222 0.824 -1.774  2.229\n",
      "Condition[T.Speech]:session[T.2]  0.485    1.021  0.475 0.635 -1.516  2.487\n",
      "Group Var                         1.672    0.464                           \n",
      "===========================================================================\n",
      "\n",
      "Running mixed model for OnlyNoise...\n",
      "                   Mixed Linear Model Regression Results\n",
      "===========================================================================\n",
      "Model:                    MixedLM       Dependent Variable:       theta    \n",
      "No. Observations:         96            Method:                   REML     \n",
      "No. Groups:               16            Scale:                    4.0127   \n",
      "Min. group size:          6             Log-Likelihood:           -209.1366\n",
      "Max. group size:          6             Converged:                Yes      \n",
      "Mean group size:          6.0                                              \n",
      "---------------------------------------------------------------------------\n",
      "                                 Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "---------------------------------------------------------------------------\n",
      "Intercept                        -0.136    0.617 -0.220 0.826 -1.345  1.073\n",
      "Condition[T.Noise]                0.324    0.708  0.457 0.647 -1.064  1.712\n",
      "Condition[T.Speech]               1.665    0.708  2.351 0.019  0.277  3.053\n",
      "session[T.2]                      0.698    0.708  0.986 0.324 -0.690  2.087\n",
      "Condition[T.Noise]:session[T.2]   0.062    1.002  0.062 0.951 -1.901  2.025\n",
      "Condition[T.Speech]:session[T.2] -0.796    1.002 -0.794 0.427 -2.759  1.167\n",
      "Group Var                         2.075    0.548                           \n",
      "===========================================================================\n",
      "\n",
      "Running mixed model for OnlySpeech...\n",
      "                   Mixed Linear Model Regression Results\n",
      "===========================================================================\n",
      "Model:                    MixedLM       Dependent Variable:       theta    \n",
      "No. Observations:         96            Method:                   REML     \n",
      "No. Groups:               16            Scale:                    2.7606   \n",
      "Min. group size:          6             Log-Likelihood:           -189.7560\n",
      "Max. group size:          6             Converged:                Yes      \n",
      "Mean group size:          6.0                                              \n",
      "---------------------------------------------------------------------------\n",
      "                                 Coef.  Std.Err.   z    P>|z| [0.025 0.975]\n",
      "---------------------------------------------------------------------------\n",
      "Intercept                        -0.364    0.477 -0.763 0.446 -1.299  0.571\n",
      "Condition[T.Noise]               -0.040    0.587 -0.069 0.945 -1.192  1.111\n",
      "Condition[T.Speech]              -0.649    0.587 -1.105 0.269 -1.800  0.502\n",
      "session[T.2]                      0.729    0.587  1.241 0.215 -0.422  1.880\n",
      "Condition[T.Noise]:session[T.2]  -0.184    0.831 -0.221 0.825 -1.812  1.445\n",
      "Condition[T.Speech]:session[T.2] -0.522    0.831 -0.628 0.530 -2.150  1.106\n",
      "Group Var                         0.884    0.324                           \n",
      "===========================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grp_results = df_roi.query(\"Condition in ['Control','Noise', 'Speech']\")\n",
    "\n",
    "for roi in [\"Left\", \"Right\", \"Back\", \"Front\", \"Noise\", \"Speech\", \"Common\", \"OnlyNoise\", \"OnlySpeech\"]:\n",
    "    print(f\"Running mixed model for {roi}...\")\n",
    "    subset = grp_results[(grp_results[\"Chroma\"] == \"hbo\") & (grp_results[\"ROI\"] == roi)].copy()\n",
    "\n",
    "    # Ensure categorical variables\n",
    "    subset[\"Condition\"] = subset[\"Condition\"].astype(\"category\")\n",
    "    subset[\"session\"] = subset[\"session\"].astype(\"category\")\n",
    "    \n",
    "    subset[\"Condition\"] = subset[\"Condition\"].cat.reorder_categories([\"Control\", \"Noise\", \"Speech\"], ordered=True)\n",
    "\n",
    "\n",
    "    # Fit mixed model: Condition + Session as fixed, ID as random intercept\n",
    "    model = smf.mixedlm(\"theta ~ Condition + session + Condition*session\", subset, groups=subset[\"Subject\"])\n",
    "    result = model.fit( method=\"powell\")\n",
    "    print(result.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Left Hemisphere t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paired t-test results (Control vs Speech) - Left Hemisphere:\n",
      "t-statistic = -0.0043\n",
      "p-value = 0.9966\n",
      "Paired t-test results  (Control vs Noise) - Left Hemisphere:\n",
      "t-statistic = 1.0241\n",
      "p-value = 0.3137\n"
     ]
    }
   ],
   "source": [
    "df_left= df_roi[df_roi[\"ROI\"] == \"Left\"]\n",
    "\n",
    "# control\n",
    "df_left_control = df_left[df_left[\"Condition\"] == \"Control\"]\n",
    "df_left_control_hbo= df_left_control[df_left_control[\"Chroma\"] == \"hbo\"]\n",
    "\n",
    "\n",
    "# noise\n",
    "df_left_noise = df_left[df_left[\"Condition\"] == \"Noise\"]\n",
    "df_left_noise_hbo= df_left_noise[df_left_noise[\"Chroma\"] == \"hbo\"]\n",
    "\n",
    "\n",
    "# speech\n",
    "df_left_speech = df_left[df_left[\"Condition\"] == \"Speech\"]\n",
    "df_left_speech_hbo= df_left_speech[df_left_speech[\"Chroma\"] == \"hbo\"]\n",
    "\n",
    "# Now lets do a paired t-test with the theta values\n",
    "from scipy.stats import ttest_rel\n",
    "import pandas as pd\n",
    "\n",
    "# Merge on Subject and session to ensure pairing\n",
    "control_speech = pd.merge(\n",
    "    df_left_control_hbo,\n",
    "    df_left_speech_hbo,\n",
    "    on=[\"Subject\", \"session\"],\n",
    "    suffixes=(\"_control\",\"_speech\")\n",
    ")\n",
    "\n",
    "# Run the paired t-test on the theta values\n",
    "t_stat, p_value = ttest_rel(control_speech[\"theta_control\"], control_speech[\"theta_speech\"])\n",
    "\n",
    "print(\"Paired t-test results (Control vs Speech) - Left Hemisphere:\")\n",
    "print(f\"t-statistic = {t_stat:.4f}\")\n",
    "print(f\"p-value = {p_value:.4f}\")\n",
    "\n",
    "# now for control and noise\n",
    "control_noise = pd.merge(\n",
    "    df_left_control_hbo,\n",
    "    df_left_noise_hbo,\n",
    "    on=[\"Subject\", \"session\"],\n",
    "    suffixes=(\"_control\",\"_noise\")\n",
    ")\n",
    "# Run the paired t-test on the theta values\n",
    "t_stat, p_value = ttest_rel(control_noise[\"theta_control\"], control_noise[\"theta_noise\"])\n",
    "print(\"Paired t-test results  (Control vs Noise) - Left Hemisphere:\")\n",
    "print(f\"t-statistic = {t_stat:.4f}\")\n",
    "print(f\"p-value = {p_value:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Right Hemisphere t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paired t-test results (Control vs Speech) - Right Hemisphere:\n",
      "t-statistic = 1.8730\n",
      "p-value = 0.3122\n",
      "Paired t-test results  (Control vs Noise) - Right Hemisphere:\n",
      "t-statistic = 2.9330\n",
      "p-value = 0.2092\n"
     ]
    }
   ],
   "source": [
    "# now for the right hemisphere\n",
    "df_right= df_roi[df_roi[\"ROI\"] == \"Right\"]\n",
    "# control\n",
    "df_right_control = df_right[df_right[\"Condition\"] == \"Control\"]\n",
    "df_right_control_hbo= df_right_control[df_right_control[\"Chroma\"] == \"hbo\"]\n",
    "# noise\n",
    "df_right_noise = df_right[df_right[\"Condition\"] == \"Noise\"]\n",
    "df_right_noise_hbo= df_right_noise[df_right_noise[\"Chroma\"] == \"hbo\"]\n",
    "# speech\n",
    "df_right_speech = df_right[df_right[\"Condition\"] == \"Speech\"]\n",
    "df_right_speech_hbo= df_right_speech[df_right_speech[\"Chroma\"] == \"hbo\"]\n",
    "# Merge on Subject and session to ensure pairing\n",
    "control_speech = pd.merge(\n",
    "    df_right_control_hbo,\n",
    "    df_right_speech_hbo,\n",
    "    on=[\"Subject\", \"session\"],\n",
    "    suffixes=(\"_control\",\"_speech\")\n",
    ")\n",
    "# Run the paired t-test on the theta values\n",
    "t_stat, p_value = ttest_rel(control_speech[\"theta_control\"], control_speech[\"theta_speech\"])\n",
    "print(\"Paired t-test results (Control vs Speech) - Right Hemisphere:\")\n",
    "print(f\"t-statistic = {t_stat:.4f}\")\n",
    "print(f\"p-value = {p_value:.4f}\")\n",
    "# now for control and noise\n",
    "control_noise = pd.merge(\n",
    "    df_right_control_hbo,\n",
    "    df_right_noise_hbo,\n",
    "    on=[\"Subject\", \"session\"],\n",
    "    suffixes=(\"_control\",\"_noise\")\n",
    ")\n",
    "# Run the paired t-test on the theta values\n",
    "t_stat, p_value = ttest_rel(control_noise[\"theta_control\"], control_noise[\"theta_noise\"])\n",
    "print(\"Paired t-test results  (Control vs Noise) - Right Hemisphere:\")\n",
    "print(f\"t-statistic = {t_stat:.4f}\")\n",
    "print(f\"p-value = {p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paired t-test results (Control vs Speech) - Back Hemisphere:\n",
      "t-statistic = 2.1853\n",
      "p-value = 0.2732\n",
      "Paired t-test results  (Control vs Noise) - Back Hemisphere:\n",
      "t-statistic = 11.1131\n",
      "p-value = 0.0571\n"
     ]
    }
   ],
   "source": [
    "# now for the back of the head\n",
    "df_back= df_roi[df_roi[\"ROI\"] == \"Back\"]\n",
    "# control\n",
    "df_back_control = df_back[df_back[\"Condition\"] == \"Control\"]\n",
    "df_back_control_hbo= df_back_control[df_back_control[\"Chroma\"] == \"hbo\"]\n",
    "# noise\n",
    "df_back_noise = df_back[df_back[\"Condition\"] == \"Noise\"]\n",
    "df_back_noise_hbo= df_back_noise[df_back_noise[\"Chroma\"] == \"hbo\"]\n",
    "# speech\n",
    "df_back_speech = df_back[df_back[\"Condition\"] == \"Speech\"]\n",
    "df_back_speech_hbo= df_back_speech[df_back_speech[\"Chroma\"] == \"hbo\"]\n",
    "# Merge on Subject and session to ensure pairing\n",
    "control_speech = pd.merge(\n",
    "    df_back_control_hbo,\n",
    "    df_back_speech_hbo,\n",
    "    on=[\"Subject\", \"session\"],\n",
    "    suffixes=(\"_control\",\"_speech\")\n",
    ")\n",
    "# Run the paired t-test on the theta values\n",
    "t_stat, p_value = ttest_rel(control_speech[\"theta_control\"], control_speech[\"theta_speech\"])\n",
    "print(\"Paired t-test results (Control vs Speech) - Back Hemisphere:\")\n",
    "print(f\"t-statistic = {t_stat:.4f}\")\n",
    "print(f\"p-value = {p_value:.4f}\")\n",
    "# now for control and noise\n",
    "control_noise = pd.merge(\n",
    "    df_back_control_hbo,\n",
    "    df_back_noise_hbo,\n",
    "    on=[\"Subject\", \"session\"],\n",
    "    suffixes=(\"_control\",\"_noise\")\n",
    ")\n",
    "# Run the paired t-test on the theta values\n",
    "t_stat, p_value = ttest_rel(control_noise[\"theta_control\"], control_noise[\"theta_noise\"])\n",
    "print(\"Paired t-test results  (Control vs Noise) - Back Hemisphere:\")\n",
    "print(f\"t-statistic = {t_stat:.4f}\")\n",
    "print(f\"p-value = {p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Front t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paired t-test results (Control vs Speech) - Front Hemisphere:\n",
      "t-statistic = 2.5492\n",
      "p-value = 0.2380\n",
      "Paired t-test results  (Control vs Noise) - Front Hemisphere:\n",
      "t-statistic = 2.8886\n",
      "p-value = 0.2122\n"
     ]
    }
   ],
   "source": [
    "# now for the front of the head\n",
    "df_front= df_roi[df_roi[\"ROI\"] == \"Front\"]\n",
    "# control\n",
    "df_front_control = df_front[df_front[\"Condition\"] == \"Control\"]\n",
    "df_front_control_hbo= df_front_control[df_front_control[\"Chroma\"] == \"hbo\"]\n",
    "# noise\n",
    "df_front_noise = df_front[df_front[\"Condition\"] == \"Noise\"]\n",
    "df_front_noise_hbo= df_front_noise[df_front_noise[\"Chroma\"] == \"hbo\"]\n",
    "# speech\n",
    "df_front_speech = df_front[df_front[\"Condition\"] == \"Speech\"]\n",
    "df_front_speech_hbo= df_front_speech[df_front_speech[\"Chroma\"] == \"hbo\"]\n",
    "# Merge on Subject and session to ensure pairing\n",
    "control_speech = pd.merge(\n",
    "    df_front_control_hbo,\n",
    "    df_front_speech_hbo,\n",
    "    on=[\"Subject\", \"session\"],\n",
    "    suffixes=(\"_control\",\"_speech\")\n",
    ")\n",
    "# Run the paired t-test on the theta values\n",
    "t_stat, p_value = ttest_rel(control_speech[\"theta_control\"], control_speech[\"theta_speech\"])\n",
    "print(\"Paired t-test results (Control vs Speech) - Front Hemisphere:\")\n",
    "print(f\"t-statistic = {t_stat:.4f}\")\n",
    "print(f\"p-value = {p_value:.4f}\")\n",
    "# now for control and noise\n",
    "control_noise = pd.merge(\n",
    "    df_front_control_hbo,\n",
    "    df_front_noise_hbo,\n",
    "    on=[\"Subject\", \"session\"],\n",
    "    suffixes=(\"_control\",\"_noise\")\n",
    ")\n",
    "# Run the paired t-test on the theta values\n",
    "t_stat, p_value = ttest_rel(control_noise[\"theta_control\"], control_noise[\"theta_noise\"])\n",
    "print(\"Paired t-test results  (Control vs Noise) - Front Hemisphere:\")\n",
    "print(f\"t-statistic = {t_stat:.4f}\")\n",
    "print(f\"p-value = {p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All optodes t-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paired t-test results (Control vs Speech) - All Channels:\n",
      "t-statistic = 29.9237\n",
      "p-value = 1.8933537288353043e-141\n",
      "Paired t-test results  (Control vs Noise) - All Channels:\n",
      "t-statistic = 29.0904\n",
      "p-value = 1.0564730214348262e-135\n",
      "Paired t-test results (Noise vs Speech) - All Channels:\n",
      "t-statistic = 8.0601\n",
      "p-value = 2.1371514858235485e-15\n"
     ]
    }
   ],
   "source": [
    "# now I want to consider all the channels as one ROI\n",
    "df_all_Control= df_cha[df_cha[\"Condition\"] == \"Control\"]\n",
    "df_all_Control_hbo= df_all_Control[df_all_Control[\"Chroma\"] == \"hbo\"]\n",
    "# noise\n",
    "df_all_noise = df_cha[df_cha[\"Condition\"] == \"Noise\"]\n",
    "df_all_noise_hbo= df_all_noise[df_all_noise[\"Chroma\"] == \"hbo\"]\n",
    "# speech\n",
    "df_all_speech = df_cha[df_cha[\"Condition\"] == \"Speech\"]\n",
    "df_all_speech_hbo= df_all_speech[df_all_speech[\"Chroma\"] == \"hbo\"]\n",
    "# Merge on Subject and session to ensure pairing\n",
    "control_speech = pd.merge(\n",
    "    df_all_Control_hbo,\n",
    "    df_all_speech_hbo,\n",
    "    on=[\"Subject\", \"session\"],\n",
    "    suffixes=(\"_control\",\"_speech\")\n",
    ")\n",
    "# Run the paired t-test on the theta values\n",
    "t_stat, p_value = ttest_rel(control_speech[\"theta_control\"], control_speech[\"theta_speech\"])\n",
    "print(\"Paired t-test results (Control vs Speech) - All Channels:\")\n",
    "print(f\"t-statistic = {t_stat:.4f}\")\n",
    "print(f\"p-value = {p_value}\")\n",
    "# now for control and noise\n",
    "control_noise = pd.merge(\n",
    "    df_all_Control_hbo,\n",
    "    df_all_noise_hbo,\n",
    "    on=[\"Subject\", \"session\"],\n",
    "    suffixes=(\"_control\",\"_noise\")\n",
    ")\n",
    "# Run the paired t-test on the theta values\n",
    "t_stat, p_value = ttest_rel(control_noise[\"theta_control\"], control_noise[\"theta_noise\"])\n",
    "print(\"Paired t-test results  (Control vs Noise) - All Channels:\")\n",
    "print(f\"t-statistic = {t_stat:.4f}\")\n",
    "print(f\"p-value = {p_value}\")\n",
    "\n",
    "# now for noise vs speech\n",
    "noise_speech = pd.merge(\n",
    "    df_all_noise_hbo,\n",
    "    df_all_speech_hbo,\n",
    "    on=[\"Subject\", \"session\"],\n",
    "    suffixes=(\"_noise\",\"_speech\")\n",
    ")\n",
    "# Run the paired t-test on the theta values\n",
    "t_stat, p_value = ttest_rel(noise_speech[\"theta_noise\"], noise_speech[\"theta_speech\"])\n",
    "print(\"Paired t-test results (Noise vs Speech) - All Channels:\")\n",
    "print(f\"t-statistic = {t_stat:.4f}\")\n",
    "print(f\"p-value = {p_value}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Channel Specific T-tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cha_channels= df_cha.copy()\n",
    "df_cha_channels[\"ch_name_ind\"] = df_cha[\"ch_name\"].str.extract(r\"^(\\S+)\")\n",
    "\n",
    "df_cha_Control= df_cha_channels[df_cha_channels[\"Condition\"] == \"Control\"]\n",
    "df_cha_Control_hbo= df_cha_Control[df_cha_Control[\"Chroma\"] == \"hbo\"]\n",
    "# noise\n",
    "df_cha_noise = df_cha_channels[df_cha_channels[\"Condition\"] == \"Noise\"]\n",
    "df_cha_noise_hbo= df_cha_noise[df_cha_noise[\"Chroma\"] == \"hbo\"]\n",
    "\n",
    "# speech\n",
    "df_cha_speech = df_cha_channels[df_cha_channels[\"Condition\"] == \"Speech\"]\n",
    "df_cha_speech_hbo= df_cha_speech[df_cha_speech[\"Chroma\"] == \"hbo\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Optode         Comparison  n  t_statistic   p_value  Significant\n",
      "0    S4_D2   Control vs Noise  2    -0.385428  0.765799        False\n",
      "1    S4_D2  Control vs Speech  2     0.745967  0.591981        False\n",
      "2    S4_D2    Noise vs Speech  2     5.926924  0.106409        False\n",
      "3    S6_D6   Control vs Noise  2     5.101371  0.123231        False\n",
      "4    S6_D6  Control vs Speech  2     1.669155  0.343623        False\n",
      "..     ...                ... ..          ...       ...          ...\n",
      "58   S3_D2  Control vs Speech  2     2.072557  0.286190        False\n",
      "59   S3_D2    Noise vs Speech  2     4.029342  0.154867        False\n",
      "60  S12_D1   Control vs Noise  2     1.610333  0.353776        False\n",
      "61  S12_D1  Control vs Speech  2     1.374085  0.400506        False\n",
      "62  S12_D1    Noise vs Speech  2     0.835560  0.556880        False\n",
      "\n",
      "[63 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "optodes = [\"S4_D2\", \"S6_D6\", \"S4_D3\", \"S5_D2\", \"S5_D3\", \"S5_D4\", \"S5_D5\", \n",
    "           \"S10_D9\", \"S10_D10\", \"S10_D11\", \"S10_D12\", \"S11_D11\", \"S11_D12\", \n",
    "           \"S6_D8\", \"S7_D6\", \"S7_D7\", \"S7_D8\", \"S8_D7\", \"S8_D8\", \"S9_D8\", \n",
    "           \"S1_D1\", \"S2_D1\", \"S3_D1\", \"S3_D2\", \"S12_D1\"]\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "# Perform paired t-tests per optode\n",
    "for opt in optodes:\n",
    "    df_c = df_cha_Control_hbo[df_cha_Control_hbo[\"ch_name_ind\"] == opt]\n",
    "    #print(df_c)\n",
    "    df_n = df_cha_noise_hbo[df_cha_noise_hbo[\"ch_name_ind\"] == opt]\n",
    "    df_s = df_cha_speech_hbo[df_cha_speech_hbo[\"ch_name_ind\"] == opt]\n",
    "\n",
    "    for cond1, cond2, name1, name2 in [(df_c, df_n, \"Control\", \"Noise\"), \n",
    "                                       (df_c, df_s, \"Control\", \"Speech\"), \n",
    "                                       (df_n, df_s, \"Noise\", \"Speech\")]:\n",
    "        merged = pd.merge(cond1[[\"ID\", \"theta\"]], cond2[[\"ID\", \"theta\"]], on=\"ID\", suffixes=(\"_1\", \"_2\"))\n",
    "        if len(merged) > 1:\n",
    "            t_stat, p_val = ttest_rel(merged[\"theta_1\"], merged[\"theta_2\"])\n",
    "            results.append({\n",
    "                \"Optode\": opt,\n",
    "                \"Comparison\": f\"{name1} vs {name2}\",\n",
    "                \"n\": len(merged),\n",
    "                \"t_statistic\": t_stat,\n",
    "                \"p_value\": p_val\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "# Add a column to indicate significance\n",
    "results_df[\"Significant\"] = results_df[\"p_value\"] < 0.05\n",
    "\n",
    "# Display the updated results\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['S5_D2', 'S5_D5', 'S10_D12', 'S8_D8', 'S9_D8', 'S1_D1', 'S3_D1'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do a new dataframe with the results that are significant\n",
    "significant_results = results_df[results_df[\"Significant\"]]\n",
    "significant_results['Optode'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_results_control_noise = significant_results[significant_results[\"Comparison\"] == \"Control vs Noise\"]\n",
    "significant_results_control_speech = significant_results[significant_results[\"Comparison\"] == \"Control vs Speech\"]\n",
    "significant_results_noise_speech = significant_results[significant_results[\"Comparison\"] == \"Noise vs Speech\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['S5_D2', 'S5_D5', 'S9_D8', 'S1_D1'], dtype=object)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "significant_results_control_noise['Optode'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['S10_D12', 'S8_D8', 'S3_D1'], dtype=object)"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "significant_results_control_speech['Optode'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=object)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "significant_results_noise_speech['Optode'].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
